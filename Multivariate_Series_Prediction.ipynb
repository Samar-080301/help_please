{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d4d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cd3043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-21</td>\n",
       "      <td>6.42</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.48</td>\n",
       "      <td>6.448930</td>\n",
       "      <td>98821100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>6.48</td>\n",
       "      <td>6.48</td>\n",
       "      <td>6.27</td>\n",
       "      <td>6.41</td>\n",
       "      <td>6.379265</td>\n",
       "      <td>69586200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>6.79</td>\n",
       "      <td>6.94</td>\n",
       "      <td>6.79</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.767395</td>\n",
       "      <td>129510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-27</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.09</td>\n",
       "      <td>7.29</td>\n",
       "      <td>7.255046</td>\n",
       "      <td>147750900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>7.49</td>\n",
       "      <td>7.49</td>\n",
       "      <td>6.74</td>\n",
       "      <td>6.78</td>\n",
       "      <td>6.747491</td>\n",
       "      <td>138714800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Open  High   Low  Close  Adj Close     Volume\n",
       "0  2020-05-21  6.42  6.55  6.33   6.48   6.448930   98821100\n",
       "1  2020-05-22  6.48  6.48  6.27   6.41   6.379265   69586200\n",
       "2  2020-05-26  6.79  6.94  6.79   6.80   6.767395  129510700\n",
       "3  2020-05-27  7.25  7.43  7.09   7.29   7.255046  147750900\n",
       "4  2020-05-28  7.49  7.49  6.74   6.78   6.747491  138714800"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('GE.csv') #data can be downloaded from here https://finance.yahoo.com/quote/GE/history?p=GE\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a65176d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2020-05-21\n",
       "1     2020-05-22\n",
       "2     2020-05-26\n",
       "3     2020-05-27\n",
       "4     2020-05-28\n",
       "         ...    \n",
       "248   2021-05-17\n",
       "249   2021-05-18\n",
       "250   2021-05-19\n",
       "251   2021-05-20\n",
       "252   2021-05-21\n",
       "Name: Date, Length: 253, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dates = pd.to_datetime(df['Date'])\n",
    "train_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a7808b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date          object\n",
       "Open         float64\n",
       "High         float64\n",
       "Low          float64\n",
       "Close        float64\n",
       "Adj Close    float64\n",
       "Volume         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27c7e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.42</td>\n",
       "      <td>6.550</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.48</td>\n",
       "      <td>6.480</td>\n",
       "      <td>6.27</td>\n",
       "      <td>6.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.79</td>\n",
       "      <td>6.940</td>\n",
       "      <td>6.79</td>\n",
       "      <td>6.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.25</td>\n",
       "      <td>7.430</td>\n",
       "      <td>7.09</td>\n",
       "      <td>7.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.49</td>\n",
       "      <td>7.490</td>\n",
       "      <td>6.74</td>\n",
       "      <td>6.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>13.23</td>\n",
       "      <td>13.340</td>\n",
       "      <td>13.12</td>\n",
       "      <td>13.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>13.18</td>\n",
       "      <td>13.270</td>\n",
       "      <td>12.97</td>\n",
       "      <td>12.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>12.90</td>\n",
       "      <td>13.100</td>\n",
       "      <td>12.79</td>\n",
       "      <td>13.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>13.09</td>\n",
       "      <td>13.110</td>\n",
       "      <td>12.92</td>\n",
       "      <td>13.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>13.11</td>\n",
       "      <td>13.275</td>\n",
       "      <td>13.05</td>\n",
       "      <td>13.215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open    High    Low   Close\n",
       "0     6.42   6.550   6.33   6.480\n",
       "1     6.48   6.480   6.27   6.410\n",
       "2     6.79   6.940   6.79   6.800\n",
       "3     7.25   7.430   7.09   7.290\n",
       "4     7.49   7.490   6.74   6.780\n",
       "..     ...     ...    ...     ...\n",
       "248  13.23  13.340  13.12  13.150\n",
       "249  13.18  13.270  12.97  12.970\n",
       "250  12.90  13.100  12.79  13.090\n",
       "251  13.09  13.110  12.92  13.060\n",
       "252  13.11  13.275  13.05  13.215\n",
       "\n",
       "[253 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training = df[list(df)[1:5]].astype(float)\n",
    "df_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c84d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data for efficient computtation\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de58915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape == (239, 14, 4).\n",
      "trainY shape == (239, 1).\n"
     ]
    }
   ],
   "source": [
    "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "#In this example, the n_features is 2. We will make timesteps = 3. \n",
    "#With this, the resultant n_samples is 5 (as the input data has 9 rows).\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 1   # Number of days we want to predict into the future\n",
    "n_past = 14     # Number of past days we want to use to predict the future\n",
    "\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc53fced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.1265945 , -1.12810962, -1.10982967, -1.09989985],\n",
       "        [-1.10493279, -1.1531506 , -1.13174417, -1.12511127],\n",
       "        [-0.99301395, -0.98859558, -0.94181846, -0.98464764],\n",
       "        ...,\n",
       "        [-0.47674316, -0.40549846, -0.45969934, -0.3867768 ],\n",
       "        [-0.48757402, -0.53428064, -0.53640011, -0.54524859],\n",
       "        [-0.56700029, -0.61655815, -0.67519198, -0.69291548]],\n",
       "\n",
       "       [[-1.10493279, -1.1531506 , -1.13174417, -1.12511127],\n",
       "        [-0.99301395, -0.98859558, -0.94181846, -0.98464764],\n",
       "        [-0.82694083, -0.81330872, -0.83224593, -0.80816769],\n",
       "        ...,\n",
       "        [-0.48757402, -0.53428064, -0.53640011, -0.54524859],\n",
       "        [-0.56700029, -0.61655815, -0.67519198, -0.69291548],\n",
       "        [-0.89553625, -0.84192698, -0.92355637, -0.93062316]],\n",
       "\n",
       "       [[-0.99301395, -0.98859558, -0.94181846, -0.98464764],\n",
       "        [-0.82694083, -0.81330872, -0.83224593, -0.80816769],\n",
       "        [-0.74029398, -0.79184502, -0.96008055, -0.9918509 ],\n",
       "        ...,\n",
       "        [-0.56700029, -0.61655815, -0.67519198, -0.69291548],\n",
       "        [-0.89553625, -0.84192698, -0.92355637, -0.93062316],\n",
       "        [-0.79444826, -0.81330872, -0.83589835, -0.82257422]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.31395832,  1.2829791 ,  1.35555219,  1.32399821],\n",
       "        [ 1.30673775,  1.3044428 ,  1.35555219,  1.29158352],\n",
       "        [ 1.33562003,  1.3688339 ,  1.38842395,  1.41043736],\n",
       "        ...,\n",
       "        [ 1.29229661,  1.28655639,  1.33729011,  1.34200636],\n",
       "        [ 1.33200974,  1.30086552,  1.37016186,  1.30238842],\n",
       "        [ 1.31395832,  1.27582454,  1.3153756 ,  1.23755905]],\n",
       "\n",
       "       [[ 1.30673775,  1.3044428 ,  1.35555219,  1.29158352],\n",
       "        [ 1.33562003,  1.3688339 ,  1.38842395,  1.41043736],\n",
       "        [ 1.37172288,  1.31875193,  1.35189978,  1.29158352],\n",
       "        ...,\n",
       "        [ 1.33200974,  1.30086552,  1.37016186,  1.30238842],\n",
       "        [ 1.31395832,  1.27582454,  1.3153756 ,  1.23755905],\n",
       "        [ 1.21287033,  1.21501073,  1.24963208,  1.28077863]],\n",
       "\n",
       "       [[ 1.33562003,  1.3688339 ,  1.38842395,  1.41043736],\n",
       "        [ 1.37172288,  1.31875193,  1.35189978,  1.29158352],\n",
       "        [ 1.30673775,  1.2829791 ,  1.38477153,  1.32399821],\n",
       "        ...,\n",
       "        [ 1.31395832,  1.27582454,  1.3153756 ,  1.23755905],\n",
       "        [ 1.21287033,  1.21501073,  1.24963208,  1.28077863],\n",
       "        [ 1.28146575,  1.21858801,  1.29711351,  1.26997373]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1baa083d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.89553625],\n",
       "       [-0.79444826],\n",
       "       [-0.93524938],\n",
       "       [-0.65364714],\n",
       "       [-0.7366837 ],\n",
       "       [-0.87026425],\n",
       "       [-0.76556598],\n",
       "       [-0.85582311],\n",
       "       [-0.88470539],\n",
       "       [-0.95330081],\n",
       "       [-1.09771222],\n",
       "       [-1.0543888 ],\n",
       "       [-1.07244022],\n",
       "       [-1.02189623],\n",
       "       [-0.94246995],\n",
       "       [-0.96774195],\n",
       "       [-0.92441853],\n",
       "       [-0.9569111 ],\n",
       "       [-0.98218309],\n",
       "       [-0.97857281],\n",
       "       [-1.08327108],\n",
       "       [-1.0038448 ],\n",
       "       [-1.03994766],\n",
       "       [-0.88470539],\n",
       "       [-0.89553625],\n",
       "       [-0.89553625],\n",
       "       [-0.91358767],\n",
       "       [-0.93885967],\n",
       "       [-0.92080824],\n",
       "       [-0.89553625],\n",
       "       [-0.90997739],\n",
       "       [-0.97496252],\n",
       "       [-1.02550652],\n",
       "       [-0.92080824],\n",
       "       [-1.09771222],\n",
       "       [-1.18796935],\n",
       "       [-1.22768249],\n",
       "       [-1.24212363],\n",
       "       [-1.21685163],\n",
       "       [-1.15908707],\n",
       "       [-1.17352821],\n",
       "       [-1.12298422],\n",
       "       [-0.96413167],\n",
       "       [-0.96413167],\n",
       "       [-1.06521965],\n",
       "       [-1.08327108],\n",
       "       [-1.03633737],\n",
       "       [-1.10132251],\n",
       "       [-1.10854308],\n",
       "       [-1.15908707],\n",
       "       [-1.18796935],\n",
       "       [-1.1518665 ],\n",
       "       [-1.04355794],\n",
       "       [-1.0543888 ],\n",
       "       [-1.09049165],\n",
       "       [-1.09410193],\n",
       "       [-1.09410193],\n",
       "       [-1.18074878],\n",
       "       [-1.20602078],\n",
       "       [-1.13742536],\n",
       "       [-1.12298422],\n",
       "       [-1.16269735],\n",
       "       [-1.20602078],\n",
       "       [-1.21685163],\n",
       "       [-1.26378534],\n",
       "       [-1.28183677],\n",
       "       [-1.21685163],\n",
       "       [-1.24212363],\n",
       "       [-0.98579338],\n",
       "       [-0.94608024],\n",
       "       [-1.05077851],\n",
       "       [-1.13381507],\n",
       "       [-1.1771385 ],\n",
       "       [-1.24212363],\n",
       "       [-1.25656477],\n",
       "       [-1.19518992],\n",
       "       [-1.20241049],\n",
       "       [-1.22768249],\n",
       "       [-1.18074878],\n",
       "       [-1.26017506],\n",
       "       [-1.13742536],\n",
       "       [-1.12298422],\n",
       "       [-1.19880021],\n",
       "       [-1.14825621],\n",
       "       [-0.89192596],\n",
       "       [-0.94608024],\n",
       "       [-0.99301395],\n",
       "       [-1.01828594],\n",
       "       [-1.02550652],\n",
       "       [-0.9316391 ],\n",
       "       [-0.77639683],\n",
       "       [-0.79083798],\n",
       "       [-0.81610997],\n",
       "       [-0.79805855],\n",
       "       [-0.58144143],\n",
       "       [-0.75112484],\n",
       "       [-0.77278655],\n",
       "       [-0.73307341],\n",
       "       [-0.67891913],\n",
       "       [-0.79444826],\n",
       "       [-0.7366837 ],\n",
       "       [-0.68252942],\n",
       "       [-0.66808828],\n",
       "       [-0.57422086],\n",
       "       [-0.54894886],\n",
       "       [-0.06878093],\n",
       "       [-0.26012605],\n",
       "       [-0.15542777],\n",
       "       [-0.28178776],\n",
       "       [-0.2456849 ],\n",
       "       [ 0.02508649],\n",
       "       [-0.03267808],\n",
       "       [ 0.15505676],\n",
       "       [ 0.02869677],\n",
       "       [ 0.03591734],\n",
       "       [ 0.11534362],\n",
       "       [ 0.42221786],\n",
       "       [ 0.35723273],\n",
       "       [ 0.3644533 ],\n",
       "       [ 0.30668874],\n",
       "       [ 0.28502702],\n",
       "       [ 0.19476989],\n",
       "       [ 0.31029902],\n",
       "       [ 0.40777672],\n",
       "       [ 0.487203  ],\n",
       "       [ 0.461931  ],\n",
       "       [ 0.52691614],\n",
       "       [ 0.55579842],\n",
       "       [ 0.62800412],\n",
       "       [ 0.62439384],\n",
       "       [ 0.50886471],\n",
       "       [ 0.57384984],\n",
       "       [ 0.50525442],\n",
       "       [ 0.44748986],\n",
       "       [ 0.33557102],\n",
       "       [ 0.42221786],\n",
       "       [ 0.41138701],\n",
       "       [ 0.47276186],\n",
       "       [ 0.41860758],\n",
       "       [ 0.40416644],\n",
       "       [ 0.37528416],\n",
       "       [ 0.40055615],\n",
       "       [ 0.487203  ],\n",
       "       [ 0.32112988],\n",
       "       [ 0.46554129],\n",
       "       [ 0.72909211],\n",
       "       [ 0.63161441],\n",
       "       [ 0.56662927],\n",
       "       [ 0.71104068],\n",
       "       [ 0.80851838],\n",
       "       [ 0.74714353],\n",
       "       [ 0.74353325],\n",
       "       [ 0.70020983],\n",
       "       [ 0.7074304 ],\n",
       "       [ 0.67132754],\n",
       "       [ 0.51608528],\n",
       "       [ 0.57746013],\n",
       "       [ 0.89877551],\n",
       "       [ 0.55579842],\n",
       "       [ 0.67493783],\n",
       "       [ 0.52691614],\n",
       "       [ 0.47637214],\n",
       "       [ 0.48359271],\n",
       "       [ 0.61356298],\n",
       "       [ 0.6099527 ],\n",
       "       [ 0.71104068],\n",
       "       [ 0.69659954],\n",
       "       [ 0.72909211],\n",
       "       [ 0.73992296],\n",
       "       [ 0.66771726],\n",
       "       [ 0.67493783],\n",
       "       [ 0.82295952],\n",
       "       [ 0.8301801 ],\n",
       "       [ 0.8301801 ],\n",
       "       [ 0.78685667],\n",
       "       [ 0.91321666],\n",
       "       [ 1.10456177],\n",
       "       [ 1.17676748],\n",
       "       [ 1.28507603],\n",
       "       [ 1.13344405],\n",
       "       [ 1.2200909 ],\n",
       "       [ 1.30312746],\n",
       "       [ 1.28146575],\n",
       "       [ 1.56667828],\n",
       "       [ 1.537796  ],\n",
       "       [ 1.6858177 ],\n",
       "       [ 1.64610456],\n",
       "       [ 1.48364172],\n",
       "       [ 1.13705434],\n",
       "       [ 0.9745915 ],\n",
       "       [ 1.15510577],\n",
       "       [ 1.3681126 ],\n",
       "       [ 1.32117889],\n",
       "       [ 1.46198001],\n",
       "       [ 1.32117889],\n",
       "       [ 1.30673775],\n",
       "       [ 1.23453204],\n",
       "       [ 1.16232634],\n",
       "       [ 1.03235607],\n",
       "       [ 1.23092176],\n",
       "       [ 1.2200909 ],\n",
       "       [ 1.24175261],\n",
       "       [ 1.38616402],\n",
       "       [ 1.33200974],\n",
       "       [ 1.41865659],\n",
       "       [ 1.4150463 ],\n",
       "       [ 1.39338459],\n",
       "       [ 1.3428406 ],\n",
       "       [ 1.42948744],\n",
       "       [ 1.46920058],\n",
       "       [ 1.43309773],\n",
       "       [ 1.40421545],\n",
       "       [ 1.512524  ],\n",
       "       [ 1.47642115],\n",
       "       [ 1.36450231],\n",
       "       [ 1.39699488],\n",
       "       [ 1.22370119],\n",
       "       [ 1.37533317],\n",
       "       [ 1.42948744],\n",
       "       [ 1.47642115],\n",
       "       [ 1.42948744],\n",
       "       [ 1.3681126 ],\n",
       "       [ 1.31395832],\n",
       "       [ 1.30673775],\n",
       "       [ 1.33562003],\n",
       "       [ 1.37172288],\n",
       "       [ 1.30673775],\n",
       "       [ 1.32839946],\n",
       "       [ 1.30673775],\n",
       "       [ 1.36089202],\n",
       "       [ 1.3175686 ],\n",
       "       [ 1.27785546],\n",
       "       [ 1.17676748],\n",
       "       [ 1.29229661],\n",
       "       [ 1.33200974],\n",
       "       [ 1.31395832],\n",
       "       [ 1.21287033],\n",
       "       [ 1.28146575],\n",
       "       [ 1.28868632]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a581ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "trainx, testx = train_test_split(trainX)\n",
    "trainy, testy = train_test_split(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd63e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 14, 4) (179, 1) (60, 14, 4) (60, 1)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape,trainy.shape,testx.shape,testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "320a3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 14, 64)            17664     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,113\n",
      "Trainable params: 30,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "11/11 [==============================] - 35s 207ms/step - loss: 1.1424 - val_loss: 0.9218\n",
      "Epoch 2/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0606 - val_loss: 0.9056\n",
      "Epoch 3/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0138 - val_loss: 0.8955\n",
      "Epoch 4/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0611 - val_loss: 0.9019\n",
      "Epoch 5/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9558 - val_loss: 0.9094\n",
      "Epoch 6/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0244 - val_loss: 0.9085\n",
      "Epoch 7/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0040 - val_loss: 0.9045\n",
      "Epoch 8/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9977 - val_loss: 0.9075\n",
      "Epoch 9/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9384 - val_loss: 0.9073\n",
      "Epoch 10/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0026 - val_loss: 0.8996\n",
      "Epoch 11/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9592 - val_loss: 0.8998\n",
      "Epoch 12/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0288 - val_loss: 0.8954\n",
      "Epoch 13/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9632 - val_loss: 0.8913\n",
      "Epoch 14/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0771- ETA: 0s - loss: 1.1 - 0s 25ms/step - loss: 1.0719 - val_loss: 0.8914\n",
      "Epoch 15/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9810 - val_loss: 0.8886\n",
      "Epoch 16/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0387 - val_loss: 0.8824\n",
      "Epoch 17/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 1.0036 - val_loss: 0.8834\n",
      "Epoch 18/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9428 - val_loss: 0.8892\n",
      "Epoch 19/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9705 - val_loss: 0.8908\n",
      "Epoch 20/500\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.9941 - val_loss: 0.9037\n",
      "Epoch 21/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9754- ETA: 0s - loss: 0.9 - 0s 26ms/step - loss: 0.9813 - val_loss: 0.9043\n",
      "Epoch 22/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.998 - 0s 26ms/step - loss: 0.9965 - val_loss: 0.8902\n",
      "Epoch 23/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.989 - ETA: 0s - loss: 0.990 - 0s 26ms/step - loss: 0.9925 - val_loss: 0.8844\n",
      "Epoch 24/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 1.0043 - val_loss: 0.8866\n",
      "Epoch 25/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0173 - val_loss: 0.8849\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0072 - val_loss: 0.8856\n",
      "Epoch 27/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9787 - val_loss: 0.8866\n",
      "Epoch 28/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0018 - val_loss: 0.8973\n",
      "Epoch 29/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0487 - val_loss: 0.9000\n",
      "Epoch 30/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9455 - val_loss: 0.8928\n",
      "Epoch 31/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9844 - val_loss: 0.8881\n",
      "Epoch 32/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9910 - val_loss: 0.8873\n",
      "Epoch 33/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0180 - val_loss: 0.8885\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9939 - val_loss: 0.8856\n",
      "Epoch 35/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0150 - val_loss: 0.8835\n",
      "Epoch 36/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0324 - val_loss: 0.8840\n",
      "Epoch 37/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9818 - val_loss: 0.8812\n",
      "Epoch 38/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9692 - val_loss: 0.8798\n",
      "Epoch 39/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9675 - val_loss: 0.8804\n",
      "Epoch 40/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 1.0102 - val_loss: 0.8770\n",
      "Epoch 41/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9754 - val_loss: 0.8778\n",
      "Epoch 42/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0966 - val_loss: 0.8758\n",
      "Epoch 43/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9710 - val_loss: 0.8784\n",
      "Epoch 44/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 1.0965 - val_loss: 0.8863\n",
      "Epoch 45/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9748 - val_loss: 0.8887\n",
      "Epoch 46/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9869 - val_loss: 0.8849\n",
      "Epoch 47/500\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 1.0231 - val_loss: 0.8973\n",
      "Epoch 48/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 1.0330 - val_loss: 0.9149\n",
      "Epoch 49/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.056 - ETA: 0s - loss: 1.044 - 0s 24ms/step - loss: 1.0374 - val_loss: 0.8880\n",
      "Epoch 50/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.061 - 0s 24ms/step - loss: 1.0519 - val_loss: 0.8861\n",
      "Epoch 51/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9822 - val_loss: 0.8924\n",
      "Epoch 52/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9884 - val_loss: 0.9011\n",
      "Epoch 53/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0045 - val_loss: 0.8983\n",
      "Epoch 54/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0254 - val_loss: 0.8945\n",
      "Epoch 55/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9832 - val_loss: 0.9002\n",
      "Epoch 56/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9311 - val_loss: 0.8947\n",
      "Epoch 57/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9915 - val_loss: 0.8945\n",
      "Epoch 58/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9878 - val_loss: 0.8884\n",
      "Epoch 59/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0620 - val_loss: 0.8815\n",
      "Epoch 60/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0182 - val_loss: 0.8792\n",
      "Epoch 61/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0509 - val_loss: 0.8708\n",
      "Epoch 62/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0093 - val_loss: 0.8702\n",
      "Epoch 63/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0317 - val_loss: 0.8803\n",
      "Epoch 64/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0082 - val_loss: 0.8838\n",
      "Epoch 65/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9788 - val_loss: 0.8863\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0134 - val_loss: 0.8866\n",
      "Epoch 67/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0540 - val_loss: 0.8869\n",
      "Epoch 68/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0465 - val_loss: 0.8856\n",
      "Epoch 69/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0117 - val_loss: 0.8833\n",
      "Epoch 70/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0575 - val_loss: 0.8788\n",
      "Epoch 71/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9573 - val_loss: 0.8751\n",
      "Epoch 72/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9654 - val_loss: 0.8739\n",
      "Epoch 73/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9680 - val_loss: 0.8754\n",
      "Epoch 74/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9923 - val_loss: 0.8750\n",
      "Epoch 75/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.9758 - val_loss: 0.8724\n",
      "Epoch 76/500\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.9490 - val_loss: 0.8711\n",
      "Epoch 77/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.071 - ETA: 0s - loss: 1.055 - 0s 26ms/step - loss: 1.0486 - val_loss: 0.8817\n",
      "Epoch 78/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9545 - val_loss: 0.8806\n",
      "Epoch 79/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9890 - val_loss: 0.8814\n",
      "Epoch 80/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0811 - val_loss: 0.8768\n",
      "Epoch 81/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0108 - val_loss: 0.8771\n",
      "Epoch 82/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0125 - val_loss: 0.8788\n",
      "Epoch 83/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9271 - val_loss: 0.8808\n",
      "Epoch 84/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9178 - val_loss: 0.8930\n",
      "Epoch 85/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9759 - val_loss: 0.8819\n",
      "Epoch 86/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0178 - val_loss: 0.8697\n",
      "Epoch 87/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0824 - val_loss: 0.8660\n",
      "Epoch 88/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0356 - val_loss: 0.8641\n",
      "Epoch 89/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9570 - val_loss: 0.8633\n",
      "Epoch 90/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9772 - val_loss: 0.8719\n",
      "Epoch 91/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9807 - val_loss: 0.8750\n",
      "Epoch 92/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9790 - val_loss: 0.8709\n",
      "Epoch 93/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0227 - val_loss: 0.8694\n",
      "Epoch 94/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9857 - val_loss: 0.8604\n",
      "Epoch 95/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0397 - val_loss: 0.8667\n",
      "Epoch 96/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0031 - val_loss: 0.8722\n",
      "Epoch 97/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0092 - val_loss: 0.8715\n",
      "Epoch 98/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0110 - val_loss: 0.8713\n",
      "Epoch 99/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0172 - val_loss: 0.8671\n",
      "Epoch 100/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.923 - 0s 19ms/step - loss: 0.9411 - val_loss: 0.8607\n",
      "Epoch 101/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0273 - val_loss: 0.8633\n",
      "Epoch 102/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 1.0190 - val_loss: 0.8538\n",
      "Epoch 103/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.034 - 0s 31ms/step - loss: 1.0308 - val_loss: 0.8455\n",
      "Epoch 104/500\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.9135 - val_loss: 0.8388- ETA: 0s - loss: 0.89\n",
      "Epoch 105/500\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.9935 - val_loss: 0.8406\n",
      "Epoch 106/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.029 - 0s 26ms/step - loss: 1.0239 - val_loss: 0.8378\n",
      "Epoch 107/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9431 - val_loss: 0.8351\n",
      "Epoch 108/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9756 - val_loss: 0.8470\n",
      "Epoch 109/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.8805 - val_loss: 0.8703\n",
      "Epoch 110/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0442 - val_loss: 0.8763\n",
      "Epoch 111/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0401 - val_loss: 0.8816\n",
      "Epoch 112/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0105 - val_loss: 0.8785\n",
      "Epoch 113/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9959 - val_loss: 0.8773\n",
      "Epoch 114/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9890 - val_loss: 0.8764\n",
      "Epoch 115/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0609 - val_loss: 0.8760\n",
      "Epoch 116/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9307 - val_loss: 0.8730\n",
      "Epoch 117/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9739 - val_loss: 0.8701\n",
      "Epoch 118/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.999 - 0s 19ms/step - loss: 0.9997 - val_loss: 0.8738\n",
      "Epoch 119/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9482 - val_loss: 0.8750\n",
      "Epoch 120/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9796 - val_loss: 0.8774\n",
      "Epoch 121/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0063 - val_loss: 0.8805\n",
      "Epoch 122/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9893- ETA: 0s - loss: 0.99 - 0s 25ms/step - loss: 0.9908 - val_loss: 0.8794\n",
      "Epoch 123/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9202 - val_loss: 0.8779\n",
      "Epoch 124/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9707 - val_loss: 0.8755\n",
      "Epoch 125/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.9887 - val_loss: 0.8767\n",
      "Epoch 126/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0120 - val_loss: 0.8774\n",
      "Epoch 127/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0527 - val_loss: 0.8759\n",
      "Epoch 128/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0067 - val_loss: 0.8776\n",
      "Epoch 129/500\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.9656 - val_loss: 0.8767- ETA: 0s - loss: 0.962\n",
      "Epoch 130/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9185- ETA: 0s - loss: 0.875 - ETA: 0s - loss: 0.89 - 0s 23ms/step - loss: 0.9319 - val_loss: 0.8776\n",
      "Epoch 131/500\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.9858 - val_loss: 0.8751- ETA: 0s - loss: 0.980\n",
      "Epoch 132/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0091- ETA: 0s - loss: 1.00 - ETA: 0s - loss: 1.009 - 0s 24ms/step - loss: 1.0089 - val_loss: 0.8746\n",
      "Epoch 133/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9895 - val_loss: 0.8747\n",
      "Epoch 134/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.0151 - val_loss: 0.8689\n",
      "Epoch 135/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 1.0313 - val_loss: 0.8645\n",
      "Epoch 136/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9368 - val_loss: 0.8641\n",
      "Epoch 137/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0898 - val_loss: 0.8712\n",
      "Epoch 138/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9491 - val_loss: 0.8708\n",
      "Epoch 139/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0664 - val_loss: 0.8714\n",
      "Epoch 140/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9467 - val_loss: 0.8687\n",
      "Epoch 141/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9903 - val_loss: 0.8668\n",
      "Epoch 142/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9720 - val_loss: 0.8657\n",
      "Epoch 143/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.9898 - val_loss: 0.8647\n",
      "Epoch 144/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.020 - 0s 18ms/step - loss: 1.0191 - val_loss: 0.8609\n",
      "Epoch 145/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9743 - val_loss: 0.8675\n",
      "Epoch 146/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0243 - val_loss: 0.8693\n",
      "Epoch 147/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 23ms/step - loss: 1.0233 - val_loss: 0.8671\n",
      "Epoch 148/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.082 - 0s 23ms/step - loss: 1.0685 - val_loss: 0.8635\n",
      "Epoch 149/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.0206 - val_loss: 0.8594\n",
      "Epoch 150/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9225 - val_loss: 0.8581\n",
      "Epoch 151/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0396 - val_loss: 0.8615\n",
      "Epoch 152/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9711 - val_loss: 0.8634\n",
      "Epoch 153/500\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.9803 - val_loss: 0.8605\n",
      "Epoch 154/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.9496 - val_loss: 0.8563\n",
      "Epoch 155/500\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.9477 - val_loss: 0.8630\n",
      "Epoch 156/500\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 1.0285 - val_loss: 0.8629\n",
      "Epoch 157/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 1.0283 - val_loss: 0.8626\n",
      "Epoch 158/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9937 - val_loss: 0.8582- ETA: 0s - loss: 0.991\n",
      "Epoch 159/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.998 - 0s 19ms/step - loss: 0.9947 - val_loss: 0.8576\n",
      "Epoch 160/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0205- ETA: 0s - loss: 1.03 - 0s 18ms/step - loss: 1.0130 - val_loss: 0.8571\n",
      "Epoch 161/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9636 - val_loss: 0.8499\n",
      "Epoch 162/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0929 - val_loss: 0.8389\n",
      "Epoch 163/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0121 - val_loss: 0.8399\n",
      "Epoch 164/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9655 - val_loss: 0.8411\n",
      "Epoch 165/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9597 - val_loss: 0.8395\n",
      "Epoch 166/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9532 - val_loss: 0.8377\n",
      "Epoch 167/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9976 - val_loss: 0.8386\n",
      "Epoch 168/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9619 - val_loss: 0.8354\n",
      "Epoch 169/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 1.0198 - val_loss: 0.8298\n",
      "Epoch 170/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0031 - val_loss: 0.8313\n",
      "Epoch 171/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0865 - val_loss: 0.8425\n",
      "Epoch 172/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0030 - val_loss: 0.8427\n",
      "Epoch 173/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.0232 - val_loss: 0.8576\n",
      "Epoch 174/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9845 - val_loss: 0.8675\n",
      "Epoch 175/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9724 - val_loss: 0.8665\n",
      "Epoch 176/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.9198 - val_loss: 0.8559- ETA: 0s - loss: 0.904\n",
      "Epoch 177/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9209 - val_loss: 0.8494\n",
      "Epoch 178/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9947 - val_loss: 0.8506- ETA: 0s - loss: 0.994\n",
      "Epoch 179/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 1.0016 - val_loss: 0.8478 ETA: 0s - loss: 1.003\n",
      "Epoch 180/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9843 - val_loss: 0.8458\n",
      "Epoch 181/500\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 1.0039 - val_loss: 0.8480\n",
      "Epoch 182/500\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.9890 - val_loss: 0.8528\n",
      "Epoch 183/500\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.9222 - val_loss: 0.8557\n",
      "Epoch 184/500\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 1.0104 - val_loss: 0.8415\n",
      "Epoch 185/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0520- ETA: 0s - loss: 1.0 - 0s 28ms/step - loss: 1.0390 - val_loss: 0.8347\n",
      "Epoch 186/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0511 - val_loss: 0.8429\n",
      "Epoch 187/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0386 - val_loss: 0.8409\n",
      "Epoch 188/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9609 - val_loss: 0.8274\n",
      "Epoch 189/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9211 - val_loss: 0.8285\n",
      "Epoch 190/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9185 - val_loss: 0.8361\n",
      "Epoch 191/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9454 - val_loss: 0.8345\n",
      "Epoch 192/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0305 - val_loss: 0.8420\n",
      "Epoch 193/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9949 - val_loss: 0.8428\n",
      "Epoch 194/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9750 - val_loss: 0.8478\n",
      "Epoch 195/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9820 - val_loss: 0.8395\n",
      "Epoch 196/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0593 - val_loss: 0.8301\n",
      "Epoch 197/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9680 - val_loss: 0.8167\n",
      "Epoch 198/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9739 - val_loss: 0.8293\n",
      "Epoch 199/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9003 - val_loss: 0.8423\n",
      "Epoch 200/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.962 - 0s 20ms/step - loss: 0.9657 - val_loss: 0.8298\n",
      "Epoch 201/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9591 - val_loss: 0.8210\n",
      "Epoch 202/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9444 - val_loss: 0.8012\n",
      "Epoch 203/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9682 - val_loss: 0.8174\n",
      "Epoch 204/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9787 - val_loss: 0.8276\n",
      "Epoch 205/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9670 - val_loss: 0.8536\n",
      "Epoch 206/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0133 - val_loss: 0.8384\n",
      "Epoch 207/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0457 - val_loss: 0.8193\n",
      "Epoch 208/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9785 - val_loss: 0.8379\n",
      "Epoch 209/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9921 - val_loss: 0.8353\n",
      "Epoch 210/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9995 - val_loss: 0.8336\n",
      "Epoch 211/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9535 - val_loss: 0.8291\n",
      "Epoch 212/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.0005 - val_loss: 0.8289\n",
      "Epoch 213/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9346 - val_loss: 0.8290\n",
      "Epoch 214/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.0118 - val_loss: 0.8427\n",
      "Epoch 215/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9874 - val_loss: 0.8329\n",
      "Epoch 216/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1.0364 - val_loss: 0.8108\n",
      "Epoch 217/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.9918 - val_loss: 0.8042\n",
      "Epoch 218/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0135 - val_loss: 0.8045\n",
      "Epoch 219/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9974 - val_loss: 0.8126\n",
      "Epoch 220/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0226 - val_loss: 0.8313\n",
      "Epoch 221/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0555 - val_loss: 0.8421\n",
      "Epoch 222/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9703 - val_loss: 0.8418\n",
      "Epoch 223/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9896 - val_loss: 0.8541\n",
      "Epoch 224/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0141 - val_loss: 0.8428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0040 - val_loss: 0.8302\n",
      "Epoch 226/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9389 - val_loss: 0.8284\n",
      "Epoch 227/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9409 - val_loss: 0.8236\n",
      "Epoch 228/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0434 - val_loss: 0.8363\n",
      "Epoch 229/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0237 - val_loss: 0.8386\n",
      "Epoch 230/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.885 - ETA: 0s - loss: 0.909 - 0s 25ms/step - loss: 0.9234 - val_loss: 0.8351\n",
      "Epoch 231/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9671 - val_loss: 0.8534\n",
      "Epoch 232/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9878 - val_loss: 0.8500\n",
      "Epoch 233/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9244 - val_loss: 0.8417\n",
      "Epoch 234/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0193 - val_loss: 0.8584\n",
      "Epoch 235/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.0045 - val_loss: 0.8556\n",
      "Epoch 236/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9100- ETA: 0s - loss: 0.8 - 0s 23ms/step - loss: 0.9226 - val_loss: 0.8512\n",
      "Epoch 237/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.000 - ETA: 0s - loss: 0.994 - 0s 25ms/step - loss: 0.9934 - val_loss: 0.8118\n",
      "Epoch 238/500\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.9840 - val_loss: 0.8250\n",
      "Epoch 239/500\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 1.0019 - val_loss: 0.8272\n",
      "Epoch 240/500\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.9404 - val_loss: 0.8012\n",
      "Epoch 241/500\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 1.0024 - val_loss: 0.8022\n",
      "Epoch 242/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9989 - val_loss: 0.8226\n",
      "Epoch 243/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9644 - val_loss: 0.8499\n",
      "Epoch 244/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0030 - val_loss: 0.8415\n",
      "Epoch 245/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.0398 - val_loss: 0.8122\n",
      "Epoch 246/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0021 - val_loss: 0.8161\n",
      "Epoch 247/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9838 - val_loss: 0.8328\n",
      "Epoch 248/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9436 - val_loss: 0.8192\n",
      "Epoch 249/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9759 - val_loss: 0.8321\n",
      "Epoch 250/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0478 - val_loss: 0.8263\n",
      "Epoch 251/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0257 - val_loss: 0.8187\n",
      "Epoch 252/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9841 - val_loss: 0.8619\n",
      "Epoch 253/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9638 - val_loss: 0.8420\n",
      "Epoch 254/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9686 - val_loss: 0.8568\n",
      "Epoch 255/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9546 - val_loss: 0.8335\n",
      "Epoch 256/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9505 - val_loss: 0.8106- ETA: 0s - loss: 0.945\n",
      "Epoch 257/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9480 - val_loss: 0.9840\n",
      "Epoch 258/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0213 - val_loss: 0.8186\n",
      "Epoch 259/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9599 - val_loss: 0.8269\n",
      "Epoch 260/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.9189 - val_loss: 0.8456\n",
      "Epoch 261/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9022 - val_loss: 0.8766\n",
      "Epoch 262/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9181 - val_loss: 0.8230\n",
      "Epoch 263/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.0195 - val_loss: 0.8561\n",
      "Epoch 264/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.8978 - val_loss: 0.8533\n",
      "Epoch 265/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9809 - val_loss: 0.8897\n",
      "Epoch 266/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9648 - val_loss: 0.8557\n",
      "Epoch 267/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.8936 - val_loss: 0.8618\n",
      "Epoch 268/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9802 - val_loss: 0.8646\n",
      "Epoch 269/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.0071 - val_loss: 0.8682\n",
      "Epoch 270/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9689 - val_loss: 0.8583\n",
      "Epoch 271/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.909 - 0s 19ms/step - loss: 0.9208 - val_loss: 0.9211\n",
      "Epoch 272/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9269 - val_loss: 0.8298\n",
      "Epoch 273/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9920 - val_loss: 0.9804\n",
      "Epoch 274/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 1.1216 - val_loss: 0.8165\n",
      "Epoch 275/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9866 - val_loss: 0.8179\n",
      "Epoch 276/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.0239 - val_loss: 0.8237\n",
      "Epoch 277/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9504 - val_loss: 0.8294\n",
      "Epoch 278/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.015 - 0s 24ms/step - loss: 1.0101 - val_loss: 0.8223\n",
      "Epoch 279/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9672 - val_loss: 0.8688\n",
      "Epoch 280/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9826 - val_loss: 0.8502\n",
      "Epoch 281/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.072 - ETA: 0s - loss: 1.048 - 0s 23ms/step - loss: 1.0426 - val_loss: 0.8264\n",
      "Epoch 282/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0088 - val_loss: 0.8312\n",
      "Epoch 283/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9672 - val_loss: 0.8409\n",
      "Epoch 284/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9652 - val_loss: 0.8251\n",
      "Epoch 285/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9907 - val_loss: 0.8405\n",
      "Epoch 286/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.0019 - val_loss: 0.8281\n",
      "Epoch 287/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 1.0265 - val_loss: 0.7933\n",
      "Epoch 288/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.9781 - val_loss: 0.7928- ETA: 0s - loss: 0.977\n",
      "Epoch 289/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9640 - val_loss: 0.8171\n",
      "Epoch 290/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9520 - val_loss: 0.8595\n",
      "Epoch 291/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9374 - val_loss: 0.8505\n",
      "Epoch 292/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9850 - val_loss: 0.8430\n",
      "Epoch 293/500\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.9264 - val_loss: 0.8550\n",
      "Epoch 294/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.8880 - val_loss: 0.8284\n",
      "Epoch 295/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9713 - val_loss: 0.8183\n",
      "Epoch 296/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9501 - val_loss: 0.8765 ETA: 0s - loss: 0.950\n",
      "Epoch 297/500\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.9359 - val_loss: 0.8672\n",
      "Epoch 298/500\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.9279 - val_loss: 0.8473\n",
      "Epoch 299/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9491 - val_loss: 0.8417\n",
      "Epoch 300/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.8903 - val_loss: 0.9200\n",
      "Epoch 301/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.905 - 0s 24ms/step - loss: 0.9095 - val_loss: 0.8403\n",
      "Epoch 302/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9596 - val_loss: 0.8473\n",
      "Epoch 303/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9296 - val_loss: 0.9222\n",
      "Epoch 304/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9169 - val_loss: 0.8313 ETA: 0s - loss: 0.914\n",
      "Epoch 305/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9549 - val_loss: 0.8899\n",
      "Epoch 306/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9575 - val_loss: 0.8235\n",
      "Epoch 307/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8787 - val_loss: 0.7960\n",
      "Epoch 308/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9040 - val_loss: 0.8436\n",
      "Epoch 309/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8793 - val_loss: 0.8383\n",
      "Epoch 310/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9729 - val_loss: 0.9121\n",
      "Epoch 311/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9109 - val_loss: 0.7688\n",
      "Epoch 312/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.8947 - val_loss: 0.8545\n",
      "Epoch 313/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.8981 - val_loss: 0.8419\n",
      "Epoch 314/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 1.0121 - val_loss: 0.9025\n",
      "Epoch 315/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9827 - val_loss: 0.7765\n",
      "Epoch 316/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9687 - val_loss: 0.8138\n",
      "Epoch 317/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9785 - val_loss: 0.8580\n",
      "Epoch 318/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9096 - val_loss: 0.8149\n",
      "Epoch 319/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9642 - val_loss: 0.7974\n",
      "Epoch 320/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.8769 - val_loss: 0.8493\n",
      "Epoch 321/500\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 0.8208 - val_loss: 0.8100\n",
      "Epoch 322/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.884 - ETA: 0s - loss: 0.880 - 0s 26ms/step - loss: 0.8905 - val_loss: 0.8962\n",
      "Epoch 323/500\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.9046 - val_loss: 0.7722\n",
      "Epoch 324/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.986 - ETA: 0s - loss: 0.975 - 0s 24ms/step - loss: 0.9708 - val_loss: 0.8259\n",
      "Epoch 325/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9103 - val_loss: 0.8606\n",
      "Epoch 326/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8185 - val_loss: 0.8121\n",
      "Epoch 327/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8923 - val_loss: 0.9318\n",
      "Epoch 328/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9316 - val_loss: 0.8446\n",
      "Epoch 329/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9482 - val_loss: 0.8949\n",
      "Epoch 330/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9721 - val_loss: 0.8440\n",
      "Epoch 331/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8955 - val_loss: 1.2248\n",
      "Epoch 332/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.920 - 0s 18ms/step - loss: 0.9159 - val_loss: 0.8341\n",
      "Epoch 333/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.0164 - val_loss: 0.9687\n",
      "Epoch 334/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9072 - val_loss: 0.8508\n",
      "Epoch 335/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9630 - val_loss: 0.8737\n",
      "Epoch 336/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9237 - val_loss: 0.8456\n",
      "Epoch 337/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9062 - val_loss: 0.8329\n",
      "Epoch 338/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.7870 - val_loss: 0.8683\n",
      "Epoch 339/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9485 - val_loss: 0.8872\n",
      "Epoch 340/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8864 - val_loss: 0.8585\n",
      "Epoch 341/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.8505 - val_loss: 0.8811\n",
      "Epoch 342/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9624 - val_loss: 0.7865\n",
      "Epoch 343/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9452 - val_loss: 0.7624\n",
      "Epoch 344/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9177 - val_loss: 0.7549\n",
      "Epoch 345/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9335 - val_loss: 0.8485\n",
      "Epoch 346/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.9413 - val_loss: 0.8260\n",
      "Epoch 347/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8875 - val_loss: 0.8632\n",
      "Epoch 348/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.8965 - val_loss: 0.8694\n",
      "Epoch 349/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9225 - val_loss: 0.9016\n",
      "Epoch 350/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9108 - val_loss: 0.8506\n",
      "Epoch 351/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9161 - val_loss: 0.8307\n",
      "Epoch 352/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.930 - 0s 18ms/step - loss: 0.9262 - val_loss: 0.7644\n",
      "Epoch 353/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.8823 - val_loss: 0.9074- ETA: 0s - loss: 0.86\n",
      "Epoch 354/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8304 - val_loss: 0.9199\n",
      "Epoch 355/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8583 - val_loss: 0.9089\n",
      "Epoch 356/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9250 - val_loss: 0.8644\n",
      "Epoch 357/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.8402 - val_loss: 0.8710\n",
      "Epoch 358/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8770 - val_loss: 0.9056\n",
      "Epoch 359/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9127 - val_loss: 0.9332\n",
      "Epoch 360/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9796 - val_loss: 0.9103\n",
      "Epoch 361/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9530 - val_loss: 0.9430\n",
      "Epoch 362/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8569 - val_loss: 0.8599\n",
      "Epoch 363/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9613 - val_loss: 1.0047\n",
      "Epoch 364/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9676 - val_loss: 0.8151\n",
      "Epoch 365/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9151 - val_loss: 0.7911\n",
      "Epoch 366/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9591 - val_loss: 0.8294\n",
      "Epoch 367/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9641 - val_loss: 0.8296\n",
      "Epoch 368/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9213 - val_loss: 0.8360\n",
      "Epoch 369/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.9362 - val_loss: 0.8489\n",
      "Epoch 370/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8666 - val_loss: 0.8751\n",
      "Epoch 371/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.9188 - val_loss: 0.8939\n",
      "Epoch 372/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8786 - val_loss: 0.8573\n",
      "Epoch 373/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8921 - val_loss: 0.8819\n",
      "Epoch 374/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8707 - val_loss: 0.8455\n",
      "Epoch 375/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8872 - val_loss: 0.8823\n",
      "Epoch 376/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9075 - val_loss: 0.8553\n",
      "Epoch 377/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.8552 - val_loss: 0.8722\n",
      "Epoch 378/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8713 - val_loss: 0.9661\n",
      "Epoch 379/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.9816 - val_loss: 0.9239\n",
      "Epoch 380/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 20ms/step - loss: 0.9415 - val_loss: 0.8463\n",
      "Epoch 381/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9232 - val_loss: 0.8619- ETA: 0s - loss: 0.926\n",
      "Epoch 382/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8934 - val_loss: 0.8598\n",
      "Epoch 383/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.110 - ETA: 0s - loss: 1.045 - ETA: 0s - loss: 1.008 - 0s 24ms/step - loss: 0.9909 - val_loss: 0.8225\n",
      "Epoch 384/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.945 - ETA: 0s - loss: 0.931 - 0s 24ms/step - loss: 0.9249 - val_loss: 0.8567\n",
      "Epoch 385/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9041- ETA: 0s - loss: 0.90 - 0s 25ms/step - loss: 0.9031 - val_loss: 0.8492\n",
      "Epoch 386/500\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.8744 - val_loss: 0.8315- ETA: 0s - loss: 0.870\n",
      "Epoch 387/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.8480 - val_loss: 0.8862\n",
      "Epoch 388/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.9381 - val_loss: 0.8476\n",
      "Epoch 389/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8860 - val_loss: 0.8214\n",
      "Epoch 390/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9451 - val_loss: 0.7895\n",
      "Epoch 391/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9424 - val_loss: 0.8880\n",
      "Epoch 392/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9054 - val_loss: 0.9361\n",
      "Epoch 393/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9291 - val_loss: 0.9837- ETA: 0s - loss: 0.947\n",
      "Epoch 394/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8450 - val_loss: 0.8844\n",
      "Epoch 395/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8896 - val_loss: 0.8794\n",
      "Epoch 396/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8639 - val_loss: 0.8274\n",
      "Epoch 397/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8185 - val_loss: 0.8077\n",
      "Epoch 398/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9341 - val_loss: 0.8661\n",
      "Epoch 399/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.8033 - val_loss: 0.9156\n",
      "Epoch 400/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9520 - val_loss: 0.8928\n",
      "Epoch 401/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.8007 - val_loss: 0.9109\n",
      "Epoch 402/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.8208 - val_loss: 0.9399\n",
      "Epoch 403/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8087 - val_loss: 0.9882\n",
      "Epoch 404/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9138 - val_loss: 0.8693\n",
      "Epoch 405/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.8531 - val_loss: 0.7857\n",
      "Epoch 406/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9150 - val_loss: 0.8645\n",
      "Epoch 407/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.8802 - val_loss: 0.8654\n",
      "Epoch 408/500\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.8634 - val_loss: 0.8345\n",
      "Epoch 409/500\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.8720 - val_loss: 0.8461\n",
      "Epoch 410/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.879 - ETA: 0s - loss: 0.882 - 0s 26ms/step - loss: 0.8827 - val_loss: 0.9335\n",
      "Epoch 411/500\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.8413 - val_loss: 0.8339\n",
      "Epoch 412/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.799 - 0s 27ms/step - loss: 0.8089 - val_loss: 0.8363\n",
      "Epoch 413/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8946 - val_loss: 0.9782\n",
      "Epoch 414/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8776 - val_loss: 0.9079\n",
      "Epoch 415/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8507 - val_loss: 1.2554\n",
      "Epoch 416/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9272 - val_loss: 0.9373\n",
      "Epoch 417/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.9284 - val_loss: 1.0237\n",
      "Epoch 418/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8398 - val_loss: 0.9110\n",
      "Epoch 419/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8229 - val_loss: 1.0273\n",
      "Epoch 420/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.9069 - val_loss: 0.8228\n",
      "Epoch 421/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.8456 - val_loss: 0.8608- ETA: 0s - loss: 0.845\n",
      "Epoch 422/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.8437 - val_loss: 0.9158- ETA: 0s - loss: 0.837\n",
      "Epoch 423/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.8719 - val_loss: 0.8859\n",
      "Epoch 424/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8693 - val_loss: 0.9212\n",
      "Epoch 425/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8798 - val_loss: 0.9164\n",
      "Epoch 426/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.8760 - val_loss: 1.1036\n",
      "Epoch 427/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8437 - val_loss: 0.8997\n",
      "Epoch 428/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.7747 - val_loss: 0.9367\n",
      "Epoch 429/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.9373 - val_loss: 0.9241\n",
      "Epoch 430/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8506 - val_loss: 0.9072\n",
      "Epoch 431/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9068 - val_loss: 0.8831\n",
      "Epoch 432/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.8966 - val_loss: 0.9078\n",
      "Epoch 433/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.9357 - val_loss: 1.0003\n",
      "Epoch 434/500\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 0.8721 - val_loss: 0.9806\n",
      "Epoch 435/500\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 0.8248 - val_loss: 0.9294\n",
      "Epoch 436/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.916 - 0s 24ms/step - loss: 0.9045 - val_loss: 0.9120\n",
      "Epoch 437/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.913 - ETA: 0s - loss: 0.897 - ETA: 0s - loss: 0.884 - 0s 24ms/step - loss: 0.8771 - val_loss: 0.8723\n",
      "Epoch 438/500\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 0.9261 - val_loss: 0.9099\n",
      "Epoch 439/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.8582 - val_loss: 0.9673\n",
      "Epoch 440/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8725 - val_loss: 1.0270\n",
      "Epoch 441/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8412 - val_loss: 0.8353\n",
      "Epoch 442/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8492 - val_loss: 0.8353\n",
      "Epoch 443/500\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.8198 - val_loss: 0.8393\n",
      "Epoch 444/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8734 - val_loss: 0.8073\n",
      "Epoch 445/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8212 - val_loss: 0.8258\n",
      "Epoch 446/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8542 - val_loss: 0.8238\n",
      "Epoch 447/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8204 - val_loss: 0.8129\n",
      "Epoch 448/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.8379 - val_loss: 0.8247\n",
      "Epoch 449/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8532 - val_loss: 0.8804\n",
      "Epoch 450/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8805 - val_loss: 0.8537\n",
      "Epoch 451/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8695 - val_loss: 0.9964\n",
      "Epoch 452/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.8529 - val_loss: 1.0900\n",
      "Epoch 453/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8521 - val_loss: 0.9767\n",
      "Epoch 454/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.8674 - val_loss: 0.8082\n",
      "Epoch 455/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9322 - val_loss: 1.2324\n",
      "Epoch 456/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 0.8882 - val_loss: 0.8150\n",
      "Epoch 457/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8728 - val_loss: 0.9075\n",
      "Epoch 458/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.8685 - val_loss: 0.9559\n",
      "Epoch 459/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8511 - val_loss: 0.9525\n",
      "Epoch 460/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.8223 - val_loss: 0.8673\n",
      "Epoch 461/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.8787 - val_loss: 0.9361\n",
      "Epoch 462/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9304 - val_loss: 0.9778\n",
      "Epoch 463/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.8216 - val_loss: 0.9085\n",
      "Epoch 464/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8704 - val_loss: 0.9300\n",
      "Epoch 465/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.7845 - val_loss: 1.0415\n",
      "Epoch 466/500\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.883 - ETA: 0s - loss: 0.878 - 0s 19ms/step - loss: 0.8763 - val_loss: 0.9566\n",
      "Epoch 467/500\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.8002 - val_loss: 0.9594\n",
      "Epoch 468/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8705 - val_loss: 0.8974\n",
      "Epoch 469/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8617 - val_loss: 0.8682- ETA: 0s - loss: 0.861\n",
      "Epoch 470/500\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.8688 - val_loss: 0.8577\n",
      "Epoch 471/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.8889 - val_loss: 0.8491\n",
      "Epoch 472/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8160 - val_loss: 0.8553\n",
      "Epoch 473/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.8166 - val_loss: 0.8744\n",
      "Epoch 474/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9251 - val_loss: 0.8862\n",
      "Epoch 475/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.9043 - val_loss: 0.8079\n",
      "Epoch 476/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8195 - val_loss: 1.0024\n",
      "Epoch 477/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.9177 - val_loss: 0.8284\n",
      "Epoch 478/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.7753 - val_loss: 0.9098\n",
      "Epoch 479/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.7675 - val_loss: 1.0323\n",
      "Epoch 480/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.7727 - val_loss: 0.8871\n",
      "Epoch 481/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8587 - val_loss: 0.8213\n",
      "Epoch 482/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.7676 - val_loss: 1.0506\n",
      "Epoch 483/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8320 - val_loss: 0.8672\n",
      "Epoch 484/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.9059 - val_loss: 0.8664\n",
      "Epoch 485/500\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.7980 - val_loss: 0.8712\n",
      "Epoch 486/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.7851 - val_loss: 0.8569\n",
      "Epoch 487/500\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.8230 - val_loss: 0.8641\n",
      "Epoch 488/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.7938 - val_loss: 0.8889\n",
      "Epoch 489/500\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.7797 - val_loss: 0.9929\n",
      "Epoch 490/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8013 - val_loss: 0.8660\n",
      "Epoch 491/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.8625 - val_loss: 0.8675\n",
      "Epoch 492/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8032 - val_loss: 1.1061\n",
      "Epoch 493/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.7817 - val_loss: 0.8969\n",
      "Epoch 494/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.7996 - val_loss: 0.9641\n",
      "Epoch 495/500\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.8057 - val_loss: 0.9770\n",
      "Epoch 496/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8238 - val_loss: 1.0952\n",
      "Epoch 497/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.7644 - val_loss: 1.0677\n",
      "Epoch 498/500\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.8110 - val_loss: 0.8991\n",
      "Epoch 499/500\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.8877 - val_loss: 0.8212\n",
      "Epoch 500/500\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 0.8195 - val_loss: 0.7913\n"
     ]
    }
   ],
   "source": [
    "#building the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(trainx.shape[1], trainx.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(trainy.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainx, trainy, epochs=500, batch_size=16, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ea9a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 day input [[[ 0.44748986  0.5067087   0.50088648  0.45960663]\n",
      "  [ 0.33557102  0.41727663  0.36209461  0.4452001 ]\n",
      "  [ 0.42221786  0.37792651  0.40957604  0.38757399]\n",
      "  ...\n",
      "  [ 0.46554129  0.68557285  0.53375824  0.65769636]\n",
      "  [ 0.72909211  0.66768643  0.68350736  0.62528168]\n",
      "  [ 0.63161441  0.62118176  0.65428802  0.6504931 ]]\n",
      "\n",
      " [[ 0.47637214  0.4637813   0.46436231  0.43439521]\n",
      "  [ 0.48359271  0.58898621  0.53741066  0.61447678]\n",
      "  [ 0.61356298  0.57109979  0.62141626  0.60007026]\n",
      "  ...\n",
      "  [ 0.8301801   0.80362319  0.87343307  0.83777794]\n",
      "  [ 0.8301801   0.77142764  0.78212264  0.75494042]\n",
      "  [ 0.78685667  0.83224145  0.84421373  0.89540405]]\n",
      "\n",
      " [[ 1.37533317  1.42249314  1.42860054  1.39603084]\n",
      "  [ 1.42948744  1.39745216  1.45781989  1.44645368]\n",
      "  [ 1.47642115  1.5369662   1.52721582  1.45365694]\n",
      "  ...\n",
      "  [ 1.30673775  1.30086552  1.35189978  1.35641289]\n",
      "  [ 1.36089202  1.38672031  1.43590538  1.37442105]\n",
      "  [ 1.3175686   1.2829791   1.30441835  1.28077863]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.02869677  0.02019822  0.05163912  0.04541899]\n",
      "  [ 0.03591734  0.0452392   0.08085846  0.08143531]\n",
      "  [ 0.11534362  0.20263965  0.17947374  0.19308589]\n",
      "  ...\n",
      "  [ 0.487203    0.49955413  0.50819132  0.47761478]\n",
      "  [ 0.461931    0.47451315  0.51914857  0.5136311 ]\n",
      "  [ 0.52691614  0.63906817  0.59584934  0.66850126]]\n",
      "\n",
      " [[-0.57422086 -0.59151717 -0.53274769 -0.55605348]\n",
      "  [-0.54894886 -0.55932162 -0.5144856  -0.52003716]\n",
      "  [-0.06878093 -0.10500669 -0.27707846 -0.29313437]\n",
      "  ...\n",
      "  [ 0.03591734  0.0452392   0.08085846  0.08143531]\n",
      "  [ 0.11534362  0.20263965  0.17947374  0.19308589]\n",
      "  [ 0.42221786  0.41012206  0.37670428  0.32994789]]\n",
      "\n",
      " [[-1.19880021 -1.18176886 -1.15365868 -1.16112759]\n",
      "  [-1.14825621 -1.08518222 -1.10617725 -1.03867211]\n",
      "  [-0.89192596 -0.9420909  -0.97469022 -0.97024111]\n",
      "  ...\n",
      "  [-0.79805855 -0.69883566 -0.74824033 -0.65329753]\n",
      "  [-0.58144143 -0.59867174 -0.6605823  -0.68571222]\n",
      "  [-0.75112484 -0.76680404 -0.76285    -0.77575301]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (59,1) doesn't match the broadcast shape (59,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4746e05dfcd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#print(x_input)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} day output {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtemp_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\samar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    935\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    938\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (59,1) doesn't match the broadcast shape (59,4)"
     ]
    }
   ],
   "source": [
    "#testing the model\n",
    "x_input = testx\n",
    "temp_input=list(x_input)\n",
    "lst_output=[]\n",
    "i=0\n",
    "while(i<10):\n",
    "    \n",
    "    if(len(temp_input)>14):\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        print(\"{} day input {}\".format(i,x_input))\n",
    "        #print(x_input)\n",
    "        #x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        #print(x_input)\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        scaler.inverse_transform(yhat)\n",
    "        print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.append(yhat[0][0])\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "        lst_output.append(yhat[0][0])\n",
    "        i=i+1\n",
    "    else:\n",
    "        #x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(yhat[0])\n",
    "        temp_input.append(yhat[0][0])\n",
    "        lst_output.append(yhat[0][0])\n",
    "        i=i+1\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "print(lst_output)\n",
    "lstm = lst_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd6c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
