{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d4d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7cd3043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.58</td>\n",
       "      <td>6.58</td>\n",
       "      <td>6.553280</td>\n",
       "      <td>74748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-10</td>\n",
       "      <td>6.54</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.53</td>\n",
       "      <td>6.69</td>\n",
       "      <td>6.662834</td>\n",
       "      <td>63423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>6.76</td>\n",
       "      <td>6.82</td>\n",
       "      <td>6.64</td>\n",
       "      <td>6.70</td>\n",
       "      <td>6.672793</td>\n",
       "      <td>68492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>6.66</td>\n",
       "      <td>6.89</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.88</td>\n",
       "      <td>6.852062</td>\n",
       "      <td>69509200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>7.09</td>\n",
       "      <td>7.21</td>\n",
       "      <td>7.03</td>\n",
       "      <td>7.14</td>\n",
       "      <td>7.111006</td>\n",
       "      <td>89827100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Open  High   Low  Close  Adj Close    Volume\n",
       "0  2020-07-09  6.83  6.83  6.58   6.58   6.553280  74748000\n",
       "1  2020-07-10  6.54  6.72  6.53   6.69   6.662834  63423400\n",
       "2  2020-07-13  6.76  6.82  6.64   6.70   6.672793  68492400\n",
       "3  2020-07-14  6.66  6.89  6.57   6.88   6.852062  69509200\n",
       "4  2020-07-15  7.09  7.21  7.03   7.14   7.111006  89827100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('GE.csv') #data can be downloaded from here https://finance.yahoo.com/quote/GE/history?p=GE\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a65176d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2020-07-09\n",
       "1     2020-07-10\n",
       "2     2020-07-13\n",
       "3     2020-07-14\n",
       "4     2020-07-15\n",
       "         ...    \n",
       "247   2021-07-01\n",
       "248   2021-07-02\n",
       "249   2021-07-06\n",
       "250   2021-07-07\n",
       "251   2021-07-08\n",
       "Name: Date, Length: 252, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dates = pd.to_datetime(df['Date'])\n",
    "train_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a7808b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date          object\n",
       "Open         float64\n",
       "High         float64\n",
       "Low          float64\n",
       "Close        float64\n",
       "Adj Close    float64\n",
       "Volume         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27c7e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.83</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.58</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.54</td>\n",
       "      <td>6.72</td>\n",
       "      <td>6.53</td>\n",
       "      <td>6.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.76</td>\n",
       "      <td>6.82</td>\n",
       "      <td>6.64</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.66</td>\n",
       "      <td>6.89</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.09</td>\n",
       "      <td>7.21</td>\n",
       "      <td>7.03</td>\n",
       "      <td>7.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>13.58</td>\n",
       "      <td>13.63</td>\n",
       "      <td>13.36</td>\n",
       "      <td>13.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>13.54</td>\n",
       "      <td>13.54</td>\n",
       "      <td>13.31</td>\n",
       "      <td>13.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>13.28</td>\n",
       "      <td>13.32</td>\n",
       "      <td>12.82</td>\n",
       "      <td>12.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>12.88</td>\n",
       "      <td>13.08</td>\n",
       "      <td>12.74</td>\n",
       "      <td>12.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>12.73</td>\n",
       "      <td>12.94</td>\n",
       "      <td>12.61</td>\n",
       "      <td>12.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open   High    Low  Close\n",
       "0     6.83   6.83   6.58   6.58\n",
       "1     6.54   6.72   6.53   6.69\n",
       "2     6.76   6.82   6.64   6.70\n",
       "3     6.66   6.89   6.57   6.88\n",
       "4     7.09   7.21   7.03   7.14\n",
       "..     ...    ...    ...    ...\n",
       "247  13.58  13.63  13.36  13.48\n",
       "248  13.54  13.54  13.31  13.36\n",
       "249  13.28  13.32  12.82  12.92\n",
       "250  12.88  13.08  12.74  12.98\n",
       "251  12.73  12.94  12.61  12.87\n",
       "\n",
       "[252 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training = df[list(df)[1:5]].astype(float)\n",
    "df_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c84d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data for efficient computtation\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de58915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape == (238, 14, 4).\n",
      "trainY shape == (238, 1).\n"
     ]
    }
   ],
   "source": [
    "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "#In this example, the n_features is 2. We will make timesteps = 3. \n",
    "#With this, the resultant n_samples is 5 (as the input data has 9 rows).\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 1   # Number of days we want to predict into the future\n",
    "n_past = 14     # Number of past days we want to use to predict the future\n",
    "\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc53fced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.22993347, -1.27950828, -1.27330806, -1.31677884],\n",
       "        [-1.33110126, -1.31765121, -1.29096079, -1.27838168],\n",
       "        [-1.25435328, -1.28297582, -1.25212479, -1.27489103],\n",
       "        ...,\n",
       "        [-1.16365112, -1.18241718, -1.18857497, -1.21904061],\n",
       "        [-1.22644492, -1.2725732 , -1.23447206, -1.27140038],\n",
       "        [-1.27528455, -1.23443027, -1.23447206, -1.20856866]],\n",
       "\n",
       "       [[-1.33110126, -1.31765121, -1.29096079, -1.27838168],\n",
       "        [-1.25435328, -1.28297582, -1.25212479, -1.27489103],\n",
       "        [-1.28923872, -1.25870304, -1.27683861, -1.21205931],\n",
       "        ...,\n",
       "        [-1.22644492, -1.2725732 , -1.23447206, -1.27140038],\n",
       "        [-1.27528455, -1.23443027, -1.23447206, -1.20856866],\n",
       "        [-1.17411675, -1.22056011, -1.29449134, -1.31328819]],\n",
       "\n",
       "       [[-1.25435328, -1.28297582, -1.25212479, -1.27489103],\n",
       "        [-1.28923872, -1.25870304, -1.27683861, -1.21205931],\n",
       "        [-1.13923131, -1.14774178, -1.11443351, -1.12130238],\n",
       "        ...,\n",
       "        [-1.27528455, -1.23443027, -1.23447206, -1.20856866],\n",
       "        [-1.17411675, -1.22056011, -1.29449134, -1.31328819],\n",
       "        [-1.34505544, -1.39046954, -1.38628552, -1.42847967]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.07948306,  1.05761328,  1.10274909,  1.11271427],\n",
       "        [ 1.09692578,  1.08535359,  1.095688  ,  1.07780776],\n",
       "        [ 1.06552888,  1.03680804,  0.96152727,  0.92421912],\n",
       "        ...,\n",
       "        [ 0.97482672,  1.04374312,  1.01095491,  1.08478906],\n",
       "        [ 1.12483414,  1.07841851,  1.12040182,  1.09177037],\n",
       "        [ 1.11087996,  1.04721066,  1.10274909,  1.04988255]],\n",
       "\n",
       "       [[ 1.09692578,  1.08535359,  1.095688  ,  1.07780776],\n",
       "        [ 1.06552888,  1.03680804,  0.96152727,  0.92421912],\n",
       "        [ 0.86668184,  0.86689861,  0.90503854,  0.84742479],\n",
       "        ...,\n",
       "        [ 1.12483414,  1.07841851,  1.12040182,  1.09177037],\n",
       "        [ 1.11087996,  1.04721066,  1.10274909,  1.04988255],\n",
       "        [ 1.0201778 ,  0.97092479,  0.92975236,  0.89629391]],\n",
       "\n",
       "       [[ 1.06552888,  1.03680804,  0.96152727,  0.92421912],\n",
       "        [ 0.86668184,  0.86689861,  0.90503854,  0.84742479],\n",
       "        [ 0.88412456,  0.92584678,  0.94387454,  0.99054149],\n",
       "        ...,\n",
       "        [ 1.11087996,  1.04721066,  1.10274909,  1.04988255],\n",
       "        [ 1.0201778 ,  0.97092479,  0.92975236,  0.89629391],\n",
       "        [ 0.88063602,  0.88770385,  0.90150799,  0.91723782]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1baa083d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.17411675],\n",
       "       [-1.34505544],\n",
       "       [-1.43226905],\n",
       "       [-1.47064304],\n",
       "       [-1.48459722],\n",
       "       [-1.46017741],\n",
       "       [-1.4043607 ],\n",
       "       [-1.41831487],\n",
       "       [-1.36947525],\n",
       "       [-1.21597929],\n",
       "       [-1.21597929],\n",
       "       [-1.31365854],\n",
       "       [-1.33110126],\n",
       "       [-1.28575018],\n",
       "       [-1.34854398],\n",
       "       [-1.35552107],\n",
       "       [-1.4043607 ],\n",
       "       [-1.43226905],\n",
       "       [-1.39738361],\n",
       "       [-1.29272727],\n",
       "       [-1.3031929 ],\n",
       "       [-1.33807835],\n",
       "       [-1.34156689],\n",
       "       [-1.34156689],\n",
       "       [-1.42529196],\n",
       "       [-1.44971178],\n",
       "       [-1.38342943],\n",
       "       [-1.36947525],\n",
       "       [-1.40784924],\n",
       "       [-1.44971178],\n",
       "       [-1.46017741],\n",
       "       [-1.50552849],\n",
       "       [-1.52297121],\n",
       "       [-1.46017741],\n",
       "       [-1.48459722],\n",
       "       [-1.23691056],\n",
       "       [-1.19853656],\n",
       "       [-1.29970436],\n",
       "       [-1.37994088],\n",
       "       [-1.42180342],\n",
       "       [-1.48459722],\n",
       "       [-1.4985514 ],\n",
       "       [-1.43924614],\n",
       "       [-1.44622323],\n",
       "       [-1.47064304],\n",
       "       [-1.42529196],\n",
       "       [-1.50203994],\n",
       "       [-1.38342943],\n",
       "       [-1.36947525],\n",
       "       [-1.44273469],\n",
       "       [-1.39389506],\n",
       "       [-1.1462084 ],\n",
       "       [-1.19853656],\n",
       "       [-1.24388764],\n",
       "       [-1.26830746],\n",
       "       [-1.27528455],\n",
       "       [-1.18458239],\n",
       "       [-1.03457497],\n",
       "       [-1.04852915],\n",
       "       [-1.07294896],\n",
       "       [-1.05550624],\n",
       "       [-0.84619356],\n",
       "       [-1.01015516],\n",
       "       [-1.03108642],\n",
       "       [-0.99271243],\n",
       "       [-0.94038426],\n",
       "       [-1.05201769],\n",
       "       [-0.99620098],\n",
       "       [-0.94387281],\n",
       "       [-0.92991863],\n",
       "       [-0.83921647],\n",
       "       [-0.81479666],\n",
       "       [-0.35082023],\n",
       "       [-0.53571309],\n",
       "       [-0.4345453 ],\n",
       "       [-0.55664436],\n",
       "       [-0.52175891],\n",
       "       [-0.26011807],\n",
       "       [-0.31593478],\n",
       "       [-0.13453046],\n",
       "       [-0.25662952],\n",
       "       [-0.24965243],\n",
       "       [-0.17290445],\n",
       "       [ 0.12362184],\n",
       "       [ 0.06082804],\n",
       "       [ 0.06780512],\n",
       "       [ 0.01198841],\n",
       "       [-0.00894286],\n",
       "       [-0.09615647],\n",
       "       [ 0.01547696],\n",
       "       [ 0.10966766],\n",
       "       [ 0.18641564],\n",
       "       [ 0.16199583],\n",
       "       [ 0.22478963],\n",
       "       [ 0.25269799],\n",
       "       [ 0.32246888],\n",
       "       [ 0.31898034],\n",
       "       [ 0.20734691],\n",
       "       [ 0.27014071],\n",
       "       [ 0.20385836],\n",
       "       [ 0.14804165],\n",
       "       [ 0.03989677],\n",
       "       [ 0.12362184],\n",
       "       [ 0.1131562 ],\n",
       "       [ 0.17246146],\n",
       "       [ 0.12013329],\n",
       "       [ 0.10617912],\n",
       "       [ 0.07827076],\n",
       "       [ 0.10269057],\n",
       "       [ 0.18641564],\n",
       "       [ 0.02594259],\n",
       "       [ 0.16548437],\n",
       "       [ 0.42014813],\n",
       "       [ 0.32595743],\n",
       "       [ 0.26316362],\n",
       "       [ 0.40270541],\n",
       "       [ 0.49689611],\n",
       "       [ 0.43759085],\n",
       "       [ 0.43410231],\n",
       "       [ 0.39223977],\n",
       "       [ 0.39921686],\n",
       "       [ 0.36433142],\n",
       "       [ 0.214324  ],\n",
       "       [ 0.27362926],\n",
       "       [ 0.58410973],\n",
       "       [ 0.25269799],\n",
       "       [ 0.36781996],\n",
       "       [ 0.22478963],\n",
       "       [ 0.17595001],\n",
       "       [ 0.1829271 ],\n",
       "       [ 0.3085147 ],\n",
       "       [ 0.30502616],\n",
       "       [ 0.40270541],\n",
       "       [ 0.38875123],\n",
       "       [ 0.42014813],\n",
       "       [ 0.43061376],\n",
       "       [ 0.36084287],\n",
       "       [ 0.36781996],\n",
       "       [ 0.51085029],\n",
       "       [ 0.51782738],\n",
       "       [ 0.51782738],\n",
       "       [ 0.47596484],\n",
       "       [ 0.5980639 ],\n",
       "       [ 0.78295677],\n",
       "       [ 0.85272766],\n",
       "       [ 0.957384  ],\n",
       "       [ 0.81086512],\n",
       "       [ 0.89459019],\n",
       "       [ 0.97482672],\n",
       "       [ 0.95389545],\n",
       "       [ 1.22949048],\n",
       "       [ 1.20158212],\n",
       "       [ 1.34461245],\n",
       "       [ 1.30623846],\n",
       "       [ 1.14925395],\n",
       "       [ 0.81435367],\n",
       "       [ 0.65736916],\n",
       "       [ 0.83179639],\n",
       "       [ 1.03762052],\n",
       "       [ 0.99226944],\n",
       "       [ 1.12832268],\n",
       "       [ 0.99226944],\n",
       "       [ 0.97831527],\n",
       "       [ 0.90854437],\n",
       "       [ 0.83877348],\n",
       "       [ 0.71318588],\n",
       "       [ 0.90505583],\n",
       "       [ 0.89459019],\n",
       "       [ 0.91552146],\n",
       "       [ 1.05506325],\n",
       "       [ 1.00273508],\n",
       "       [ 1.08646015],\n",
       "       [ 1.0829716 ],\n",
       "       [ 1.06204034],\n",
       "       [ 1.01320071],\n",
       "       [ 1.09692578],\n",
       "       [ 1.13529977],\n",
       "       [ 1.10041433],\n",
       "       [ 1.07250597],\n",
       "       [ 1.17716231],\n",
       "       [ 1.14227686],\n",
       "       [ 1.03413198],\n",
       "       [ 1.06552888],\n",
       "       [ 0.89807874],\n",
       "       [ 1.04459761],\n",
       "       [ 1.09692578],\n",
       "       [ 1.14227686],\n",
       "       [ 1.09692578],\n",
       "       [ 1.03762052],\n",
       "       [ 0.98529235],\n",
       "       [ 0.97831527],\n",
       "       [ 1.00622362],\n",
       "       [ 1.04110907],\n",
       "       [ 0.97831527],\n",
       "       [ 0.99924653],\n",
       "       [ 0.97831527],\n",
       "       [ 1.03064343],\n",
       "       [ 0.9887809 ],\n",
       "       [ 0.95040691],\n",
       "       [ 0.85272766],\n",
       "       [ 0.96436109],\n",
       "       [ 1.00273508],\n",
       "       [ 0.98529235],\n",
       "       [ 0.88761311],\n",
       "       [ 0.95389545],\n",
       "       [ 0.96087254],\n",
       "       [ 1.02366634],\n",
       "       [ 0.99226944],\n",
       "       [ 0.96436109],\n",
       "       [ 1.13181123],\n",
       "       [ 1.3725208 ],\n",
       "       [ 1.35158954],\n",
       "       [ 1.33414681],\n",
       "       [ 1.26786447],\n",
       "       [ 1.32716972],\n",
       "       [ 1.27135301],\n",
       "       [ 1.26088738],\n",
       "       [ 1.19111649],\n",
       "       [ 1.20507066],\n",
       "       [ 1.14925395],\n",
       "       [ 1.19460503],\n",
       "       [ 1.07948306],\n",
       "       [ 1.09692578],\n",
       "       [ 1.06552888],\n",
       "       [ 0.86668184],\n",
       "       [ 0.88412456],\n",
       "       [ 0.96436109],\n",
       "       [ 0.92947564],\n",
       "       [ 0.94342982],\n",
       "       [ 0.97831527],\n",
       "       [ 0.97831527],\n",
       "       [ 0.94342982],\n",
       "       [ 0.97482672],\n",
       "       [ 1.12483414],\n",
       "       [ 1.11087996],\n",
       "       [ 1.0201778 ],\n",
       "       [ 0.88063602],\n",
       "       [ 0.82830785]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a581ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "trainx, testx = train_test_split(trainX)\n",
    "trainy, testy = train_test_split(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd63e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 14, 4) (178, 1) (60, 14, 4) (60, 1)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape,trainy.shape,testx.shape,testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ceb0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "320a3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 14, 64)            17664     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 30,212\n",
      "Trainable params: 30,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "10/10 [==============================] - 6s 179ms/step - loss: 1.0143 - val_loss: 0.9227\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 1.0474 - val_loss: 0.9066\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.9290 - val_loss: 0.9015\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 1.0017 - val_loss: 0.9194\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 1.0682 - val_loss: 0.9234\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9475 - val_loss: 0.9039\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9467 - val_loss: 0.9063\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 1.0218 - val_loss: 0.9211\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9555 - val_loss: 0.9178\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9422 - val_loss: 0.9106\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9276 - val_loss: 0.9134\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9572 - val_loss: 0.9126\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9679 - val_loss: 0.9075\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9346 - val_loss: 0.9064\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9735 - val_loss: 0.8946\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9958 - val_loss: 0.8945\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9826 - val_loss: 0.8982\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9974 - val_loss: 0.9005\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9492 - val_loss: 0.9029\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9509 - val_loss: 0.9094\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 1.0133 - val_loss: 0.9121\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.9769 - val_loss: 0.9104\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.9994 - val_loss: 0.9079\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9719 - val_loss: 0.9182\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9024 - val_loss: 0.9230\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9284 - val_loss: 0.9143\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.9736 - val_loss: 0.9012\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9500 - val_loss: 0.9064\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9549 - val_loss: 0.9074\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9871 - val_loss: 0.9083\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 1.0002 - val_loss: 0.9309\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.9175 - val_loss: 0.9144\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 1.0158 - val_loss: 0.9121\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9003 - val_loss: 0.9112\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9988 - val_loss: 0.9416\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9506 - val_loss: 0.9405\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.9799 - val_loss: 0.8978\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.9351 - val_loss: 0.9050\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9896 - val_loss: 0.9085\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9543 - val_loss: 0.8960\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 1.0668 - val_loss: 0.8872\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.9982 - val_loss: 0.9150\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9182 - val_loss: 0.9094\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9210 - val_loss: 0.9121\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.8777 - val_loss: 0.9188\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8755 - val_loss: 0.9092\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.9482 - val_loss: 0.9183\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9821 - val_loss: 0.9221\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9616 - val_loss: 0.9026\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9768 - val_loss: 0.9203\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 1.0170 - val_loss: 0.9091\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9071 - val_loss: 0.8991\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9455 - val_loss: 0.9030\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 1.0176 - val_loss: 0.9086\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9593 - val_loss: 0.9069\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9888 - val_loss: 0.9108\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9624 - val_loss: 0.8990\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8671 - val_loss: 0.9121\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9363 - val_loss: 0.9091\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 1.0207 - val_loss: 0.9115\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.9920 - val_loss: 0.9054\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9252 - val_loss: 0.9190\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.9848 - val_loss: 0.9226\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9784 - val_loss: 0.9191\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9507 - val_loss: 0.8879\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8955 - val_loss: 0.9180\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9065 - val_loss: 0.9369\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.9444 - val_loss: 0.9057\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8553 - val_loss: 0.9210\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 1.0097 - val_loss: 0.9129\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8993 - val_loss: 0.9044\n",
      "Epoch 72/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9349 - val_loss: 0.9260\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9395 - val_loss: 0.9313\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9828 - val_loss: 0.9242\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8608 - val_loss: 0.9134\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 1.0256 - val_loss: 0.9533\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9316 - val_loss: 0.9406\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9764 - val_loss: 0.9027\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9624 - val_loss: 0.9229\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9059 - val_loss: 0.9811\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9298 - val_loss: 0.8818\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9709 - val_loss: 0.9009\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9021 - val_loss: 0.9026\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9706 - val_loss: 0.9079\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.9822 - val_loss: 0.9081\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.9582 - val_loss: 0.9161\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8907 - val_loss: 0.9214\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9395 - val_loss: 0.9223\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9521 - val_loss: 0.9213\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.9750 - val_loss: 0.9313\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9860 - val_loss: 0.9135\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9397 - val_loss: 0.9310\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8949 - val_loss: 0.9319\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.9675 - val_loss: 0.8932\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9702 - val_loss: 0.8961\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9464 - val_loss: 0.9004\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9789 - val_loss: 0.8997\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9407 - val_loss: 0.9022\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 1.0101 - val_loss: 0.9043\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8545 - val_loss: 0.9087\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 1.0022 - val_loss: 0.9036\n",
      "Epoch 102/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8532 - val_loss: 0.9076\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9047 - val_loss: 0.9135\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9695 - val_loss: 0.9266\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 1.0233 - val_loss: 0.9263\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9173 - val_loss: 0.9151\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9264 - val_loss: 0.9191\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9790 - val_loss: 0.9276\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9768 - val_loss: 0.9325\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.9829 - val_loss: 0.9296\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.9611 - val_loss: 0.9163\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8868 - val_loss: 0.9385\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9371 - val_loss: 0.9313\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8766 - val_loss: 0.9226\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.9810 - val_loss: 0.9274\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 1.0480 - val_loss: 0.9171\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9569 - val_loss: 0.9169\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9335 - val_loss: 0.9179\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 1.0427 - val_loss: 0.9215\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.9466 - val_loss: 0.9448\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.9270 - val_loss: 0.9401\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8477 - val_loss: 0.9120\n",
      "Epoch 123/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9859 - val_loss: 0.9343\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9791 - val_loss: 0.9218\n",
      "Epoch 125/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.9366 - val_loss: 0.9216\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9516 - val_loss: 0.9190\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9918 - val_loss: 0.9181\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9264 - val_loss: 0.9333\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9913 - val_loss: 0.9273\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.9048 - val_loss: 0.9320\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 1.0065 - val_loss: 0.9443\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9508 - val_loss: 0.9300\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9416 - val_loss: 0.9289\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8959 - val_loss: 0.9158\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9064 - val_loss: 0.9226\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9125 - val_loss: 0.9188\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9473 - val_loss: 0.9558\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9803 - val_loss: 0.9441\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9480 - val_loss: 0.9332\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.8636 - val_loss: 1.0184\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.9374 - val_loss: 0.9393\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.9402 - val_loss: 0.9266\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9280 - val_loss: 0.9619\n",
      "Epoch 144/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8731 - val_loss: 0.9937\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9181 - val_loss: 0.9893\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.9427 - val_loss: 1.0950\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9689 - val_loss: 0.9480\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9132 - val_loss: 0.8804\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9908 - val_loss: 0.8954\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8420 - val_loss: 0.9031\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9421 - val_loss: 0.9266\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.9359 - val_loss: 0.9497\n",
      "Epoch 153/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 23ms/step - loss: 0.9478 - val_loss: 0.9361\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.9206 - val_loss: 0.9398\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8993 - val_loss: 0.9933\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9294 - val_loss: 0.9419\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9302 - val_loss: 0.9525\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9477 - val_loss: 0.9106\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8434 - val_loss: 1.0196\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9049 - val_loss: 0.9872\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9143 - val_loss: 1.0200\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9281 - val_loss: 1.0152\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9559 - val_loss: 0.9139\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.9429 - val_loss: 0.9742\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9018 - val_loss: 1.0480\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8671 - val_loss: 0.9611\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.8425 - val_loss: 0.9567\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8848 - val_loss: 0.9182\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9849 - val_loss: 1.0062\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9011 - val_loss: 1.0084\n",
      "Epoch 171/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9516 - val_loss: 1.0025\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9217 - val_loss: 0.9707\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8805 - val_loss: 0.9537\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8762 - val_loss: 1.0015\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9211 - val_loss: 1.0733\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9477 - val_loss: 0.9785\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.9306 - val_loss: 1.1436\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8710 - val_loss: 1.0019\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.8792 - val_loss: 1.0139\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9026 - val_loss: 0.9960\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8250 - val_loss: 0.9505\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.8873 - val_loss: 1.0525\n",
      "Epoch 183/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9492 - val_loss: 1.0311\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8535 - val_loss: 1.1488\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8672 - val_loss: 0.9690\n",
      "Epoch 186/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8878 - val_loss: 1.0450\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9358 - val_loss: 1.0585\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8398 - val_loss: 1.0947\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9282 - val_loss: 0.9921\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9448 - val_loss: 0.9576\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9042 - val_loss: 0.9478\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9451 - val_loss: 0.9432\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8635 - val_loss: 0.9844\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9326 - val_loss: 1.0194\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9250 - val_loss: 1.0502\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8204 - val_loss: 1.0950\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8760 - val_loss: 1.0101\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.8534 - val_loss: 1.0448\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.8724 - val_loss: 1.0629\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8706 - val_loss: 1.1393\n",
      "Epoch 201/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9552 - val_loss: 0.9685\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9389 - val_loss: 1.0587\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8971 - val_loss: 1.0303\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.8216 - val_loss: 0.9827\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.9470 - val_loss: 1.0582\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8625 - val_loss: 1.0372\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9510 - val_loss: 1.0659\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8601 - val_loss: 1.0370\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8615 - val_loss: 0.9953\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8624 - val_loss: 1.0609\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9186 - val_loss: 1.0400\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9319 - val_loss: 1.0421\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8280 - val_loss: 1.0026\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8519 - val_loss: 1.1796\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 1.6702 - val_loss: 1.0880\n",
      "Epoch 216/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9813 - val_loss: 1.0142\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9344 - val_loss: 0.9929\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9418 - val_loss: 0.9612\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9791 - val_loss: 0.9591\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9722 - val_loss: 1.0284\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8600 - val_loss: 0.9983\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.9531 - val_loss: 1.0336\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.9847 - val_loss: 1.1243\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8894 - val_loss: 1.0726\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9278 - val_loss: 1.0065\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.9281 - val_loss: 1.0899\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8770 - val_loss: 1.0122\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8754 - val_loss: 1.0216\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8805 - val_loss: 1.0744\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8693 - val_loss: 0.9999\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8953 - val_loss: 0.9943\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.9360 - val_loss: 1.0721\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9086 - val_loss: 1.0809\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8934 - val_loss: 1.0023\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8800 - val_loss: 1.1060\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.8447 - val_loss: 1.2392\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9081 - val_loss: 1.0623\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8667 - val_loss: 1.0266\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8303 - val_loss: 1.0663\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.9269 - val_loss: 1.1226\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.9368 - val_loss: 1.1394\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8386 - val_loss: 1.0842\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8633 - val_loss: 1.0727\n",
      "Epoch 244/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9368 - val_loss: 1.0258\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8176 - val_loss: 1.1767\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9309 - val_loss: 1.0534\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8927 - val_loss: 1.0516\n",
      "Epoch 248/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.9170 - val_loss: 1.0323\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8650 - val_loss: 1.0527\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.7949 - val_loss: 1.0842\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.8885 - val_loss: 1.1034\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9214 - val_loss: 1.1193\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9625 - val_loss: 0.9573\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8712 - val_loss: 0.9584\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.8282 - val_loss: 0.9438\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8409 - val_loss: 1.0397\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8428 - val_loss: 1.1467\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8522 - val_loss: 1.1246\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8749 - val_loss: 1.0492\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.9160 - val_loss: 1.0719\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7983 - val_loss: 1.1728\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.8861 - val_loss: 1.0692\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9470 - val_loss: 0.9771\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8365 - val_loss: 1.0073\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.8487 - val_loss: 1.1351\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8411 - val_loss: 1.0248\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.8473 - val_loss: 1.1078\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8414 - val_loss: 1.1430\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8750 - val_loss: 1.0493\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9092 - val_loss: 1.0450\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8481 - val_loss: 1.0337\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8386 - val_loss: 1.0449\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8563 - val_loss: 1.0521\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8768 - val_loss: 1.0960\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.9336 - val_loss: 0.9689\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8598 - val_loss: 0.9733\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.8470 - val_loss: 1.0602\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8179 - val_loss: 1.0562\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9351 - val_loss: 1.0465\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8140 - val_loss: 1.0617\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9047 - val_loss: 1.0427\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8232 - val_loss: 1.0109\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.9015 - val_loss: 1.0277\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8334 - val_loss: 1.0636\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8580 - val_loss: 0.9971\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8332 - val_loss: 1.0217\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.8071 - val_loss: 1.0386\n",
      "Epoch 288/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8065 - val_loss: 1.1114\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8436 - val_loss: 1.1570\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8466 - val_loss: 1.0002\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8696 - val_loss: 0.9841\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8448 - val_loss: 1.0348\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.8904 - val_loss: 1.0499\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9601 - val_loss: 1.0541\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8052 - val_loss: 1.1181\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8625 - val_loss: 1.0200\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.9032 - val_loss: 1.0358\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.9044 - val_loss: 1.1390\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.9228 - val_loss: 0.9794\n",
      "Epoch 300/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8765 - val_loss: 0.9920\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8280 - val_loss: 1.0071\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8416 - val_loss: 1.0500\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.8388 - val_loss: 1.1157\n",
      "Epoch 304/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8265 - val_loss: 1.0162\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.8380 - val_loss: 1.0095\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8028 - val_loss: 1.0709\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8768 - val_loss: 1.1229\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8826 - val_loss: 1.0505\n",
      "Epoch 309/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.7763 - val_loss: 1.0313\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8211 - val_loss: 1.0874\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.7930 - val_loss: 1.0139\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.7991 - val_loss: 0.9920\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8243 - val_loss: 0.9537\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8068 - val_loss: 0.9964\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8996 - val_loss: 1.0221\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8498 - val_loss: 1.0750\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8351 - val_loss: 1.0389\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8009 - val_loss: 1.0256\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7968 - val_loss: 1.1003\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8223 - val_loss: 1.1515\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8420 - val_loss: 1.1163\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8091 - val_loss: 1.0083\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.7887 - val_loss: 0.9939\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.8653 - val_loss: 1.0387\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8899 - val_loss: 1.0199\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8236 - val_loss: 1.0517\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8745 - val_loss: 1.1568\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8877 - val_loss: 1.0049\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.9331 - val_loss: 0.9854\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8632 - val_loss: 1.0564\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8418 - val_loss: 1.0768\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.7535 - val_loss: 1.0590\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.9065 - val_loss: 1.0236\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8789 - val_loss: 1.1326\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8532 - val_loss: 1.0284\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8337 - val_loss: 1.0893\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9282 - val_loss: 1.1329\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8828 - val_loss: 1.0853\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.8395 - val_loss: 1.1162\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8049 - val_loss: 1.1438\n",
      "Epoch 341/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8614 - val_loss: 1.0484\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8065 - val_loss: 1.0694\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8387 - val_loss: 1.1024\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.8812 - val_loss: 1.0561\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.8260 - val_loss: 1.0445\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7519 - val_loss: 1.0138\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7866 - val_loss: 1.1865\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8421 - val_loss: 1.1079\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.7717 - val_loss: 1.0321\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8579 - val_loss: 1.1209\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7378 - val_loss: 1.0388\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8097 - val_loss: 1.0739\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8720 - val_loss: 0.9948\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.8008 - val_loss: 1.1493\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8332 - val_loss: 1.0691\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8719 - val_loss: 1.0013\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8485 - val_loss: 1.0084\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8092 - val_loss: 1.0133\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8350 - val_loss: 1.1041\n",
      "Epoch 360/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8916 - val_loss: 1.0595\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8591 - val_loss: 1.0075\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8618 - val_loss: 1.0204\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8351 - val_loss: 1.0432\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8345 - val_loss: 1.0505\n",
      "Epoch 365/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8569 - val_loss: 1.0605\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8415 - val_loss: 1.0175\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8680 - val_loss: 1.0590\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.7939 - val_loss: 1.1016\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.7610 - val_loss: 1.2020\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8600 - val_loss: 1.0532\n",
      "Epoch 371/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7449 - val_loss: 1.0693\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8648 - val_loss: 0.9848\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.7678 - val_loss: 1.0320\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8610 - val_loss: 1.0469\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8675 - val_loss: 0.9741\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9205 - val_loss: 1.0605\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8189 - val_loss: 1.0916\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8788 - val_loss: 0.9870\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.7126 - val_loss: 1.0072\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8436 - val_loss: 1.0959\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7676 - val_loss: 1.1492\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8153 - val_loss: 1.0155\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.7985 - val_loss: 1.0240\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8774 - val_loss: 1.0320\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.7839 - val_loss: 1.0772\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8229 - val_loss: 1.0448\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8453 - val_loss: 0.9945\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8019 - val_loss: 0.9972\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8077 - val_loss: 1.1565\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8721 - val_loss: 1.1512\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8273 - val_loss: 1.1958\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.7937 - val_loss: 1.0745\n",
      "Epoch 393/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 13ms/step - loss: 0.7633 - val_loss: 1.0821\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8776 - val_loss: 1.1038\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8599 - val_loss: 1.1047\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8402 - val_loss: 1.2446\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.7627 - val_loss: 1.1892\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8037 - val_loss: 1.0404\n",
      "Epoch 399/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.7539 - val_loss: 1.0124\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8744 - val_loss: 1.0656\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8322 - val_loss: 1.1444\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8627 - val_loss: 0.9874\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8323 - val_loss: 1.1916\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.7557 - val_loss: 1.0546\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8438 - val_loss: 1.1276\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.9578 - val_loss: 1.0282\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8643 - val_loss: 1.0555\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.9025 - val_loss: 1.1145\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8511 - val_loss: 0.9576\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.7985 - val_loss: 0.9725\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.7887 - val_loss: 1.1821\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8244 - val_loss: 1.2186\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.9131 - val_loss: 1.0083\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7461 - val_loss: 1.0675\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8095 - val_loss: 1.0179\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8239 - val_loss: 1.0320\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8468 - val_loss: 1.1139\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8007 - val_loss: 1.0819\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7647 - val_loss: 0.9988\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.8329 - val_loss: 0.9919\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9904 - val_loss: 1.0774\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.7353 - val_loss: 1.0952\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8833 - val_loss: 1.1045\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.8334 - val_loss: 1.1864\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8672 - val_loss: 1.1350\n",
      "Epoch 426/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.7571 - val_loss: 1.1455\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8705 - val_loss: 1.2242\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8058 - val_loss: 1.0558\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8155 - val_loss: 1.0763\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8567 - val_loss: 1.1640\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.7680 - val_loss: 0.9694\n",
      "Epoch 432/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8246 - val_loss: 0.9744\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7592 - val_loss: 1.0835\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.7702 - val_loss: 1.0519\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.7891 - val_loss: 1.1312\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8286 - val_loss: 1.0589\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8659 - val_loss: 1.0968\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.7243 - val_loss: 1.0258\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.7933 - val_loss: 0.9860\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.7756 - val_loss: 1.1338\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.7371 - val_loss: 1.0619\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.7903 - val_loss: 1.0600\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.7187 - val_loss: 1.1452\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.8162 - val_loss: 1.2891\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.8149 - val_loss: 1.1067\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.7530 - val_loss: 1.1839\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.9194 - val_loss: 1.3226\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8080 - val_loss: 1.0724\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.8006 - val_loss: 1.1382\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.7958 - val_loss: 0.9951\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8273 - val_loss: 1.1729\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8032 - val_loss: 1.0973\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8516 - val_loss: 1.1417\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8232 - val_loss: 1.0304\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8358 - val_loss: 1.0080\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.7966 - val_loss: 1.1175\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.7917 - val_loss: 1.1921\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8473 - val_loss: 1.2013\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.7759 - val_loss: 1.3210\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.7768 - val_loss: 1.1483\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.7823 - val_loss: 1.1184\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8323 - val_loss: 1.1868\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8400 - val_loss: 0.9966\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.8271 - val_loss: 1.0745\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8047 - val_loss: 1.1370\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.8116 - val_loss: 1.0411\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7921 - val_loss: 0.9956\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.7923 - val_loss: 0.9926\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7934 - val_loss: 1.0901\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8038 - val_loss: 1.1647\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.8049 - val_loss: 1.0266\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.7157 - val_loss: 1.1551\n",
      "Epoch 473/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 15ms/step - loss: 0.7313 - val_loss: 1.2651\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.6589 - val_loss: 1.0218\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.8082 - val_loss: 0.9643\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.8286 - val_loss: 1.0460\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8043 - val_loss: 1.1354\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.7745 - val_loss: 1.0896\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.8249 - val_loss: 1.1586\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.8378 - val_loss: 1.2045\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.7865 - val_loss: 1.1365\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.7520 - val_loss: 1.2258\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8547 - val_loss: 1.0449\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.7114 - val_loss: 1.2321\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.7971 - val_loss: 1.0821\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 0.7670 - val_loss: 1.0986\n",
      "Epoch 487/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.7518 - val_loss: 1.1081\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.7210 - val_loss: 1.1398\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.7858 - val_loss: 1.2498\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.6928 - val_loss: 1.0268\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 0.7910 - val_loss: 1.1920\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.7961 - val_loss: 0.9573\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.7519 - val_loss: 1.0001\n",
      "Epoch 494/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.8433 - val_loss: 1.0464\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.7333 - val_loss: 1.0300\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.7802 - val_loss: 1.1226\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7788 - val_loss: 1.0842\n",
      "Epoch 498/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.6894 - val_loss: 1.0599\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.7317 - val_loss: 1.2186\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 0.8251 - val_loss: 1.0761\n"
     ]
    }
   ],
   "source": [
    "#building the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(trainx.shape[1], trainx.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainx, trainy, epochs=500, batch_size=16, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28ea9a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 day input [[[ 0.12362184  0.08323471  0.10713525  0.08995352]\n",
      "  [ 0.1131562   0.15952057  0.15303234  0.1772198 ]\n",
      "  [ 0.17246146  0.11444256  0.15303234  0.10391613]\n",
      "  ...\n",
      "  [ 0.32595743  0.31902739  0.3436818   0.34477105]\n",
      "  [ 0.26316362  0.35023524  0.30837634  0.38316821]\n",
      "  [ 0.40270541  0.48200174  0.43194544  0.49835969]]\n",
      "\n",
      " [[-1.35552107 -1.3696643  -1.35451061 -1.38659186]\n",
      "  [-1.4043607  -1.43554755 -1.40393825 -1.42498902]\n",
      "  [-1.43226905 -1.43554755 -1.39687716 -1.41102642]\n",
      "  ...\n",
      "  [-1.38342943 -1.34192399 -1.37922443 -1.40753577]\n",
      "  [-1.36947525 -1.37313184 -1.38628552 -1.37262926]\n",
      "  [-1.40784924 -1.44248263 -1.43218261 -1.47036748]]\n",
      "\n",
      " [[-1.31365854 -1.30378106 -1.26977752 -1.30979754]\n",
      "  [-1.33110126 -1.31765121 -1.31214406 -1.28885363]\n",
      "  [-1.28575018 -1.32805383 -1.31920516 -1.355176  ]\n",
      "  ...\n",
      "  [-1.34156689 -1.35232661 -1.31214406 -1.30630689]\n",
      "  [-1.34156689 -1.32805383 -1.38628552 -1.40055446]\n",
      "  [-1.42529196 -1.42861247 -1.42512152 -1.45291423]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.99226944  0.98132741  0.98624109  1.00101344]\n",
      "  [ 0.97831527  0.94318448  0.96858836  0.96959758]\n",
      "  [ 0.90854437  0.92237924  0.83442763  0.80553698]\n",
      "  ...\n",
      "  [ 1.0829716   1.0333405   1.12393236  1.06384516]\n",
      "  [ 1.06204034  1.04374312  1.08509636  1.06035451]\n",
      "  [ 1.01320071  1.0333405   1.02507709  1.08129841]]\n",
      "\n",
      " [[-1.33110126 -1.31765121 -1.31214406 -1.28885363]\n",
      "  [-1.28575018 -1.32805383 -1.31920516 -1.355176  ]\n",
      "  [-1.34854398 -1.3696643  -1.33332734 -1.36564795]\n",
      "  ...\n",
      "  [-1.34156689 -1.32805383 -1.38628552 -1.40055446]\n",
      "  [-1.42529196 -1.42861247 -1.42512152 -1.45291423]\n",
      "  [-1.44971178 -1.41127478 -1.41806043 -1.36564795]]\n",
      "\n",
      " [[-1.26830746 -1.25870304 -1.22388043 -1.23300321]\n",
      "  [-1.27528455 -1.26217058 -1.26271643 -1.21554996]\n",
      "  [-1.18458239 -1.09919623 -1.14620842 -1.06894262]\n",
      "  ...\n",
      "  [-0.94038426 -0.96396219 -1.01557824 -1.04101741]\n",
      "  [-1.05201769 -1.03331298 -1.02263933 -1.02356415]\n",
      "  [-0.99620098 -1.01944282 -1.00851715 -0.98865764]]]\n",
      "0 day output [[ 8.39941144e-01  8.35426509e-01  8.42165172e-01  8.12467277e-01]\n",
      " [-1.59430876e-01 -1.50257662e-01 -1.50414571e-01 -1.35586292e-01]\n",
      " [-1.19609520e-01 -1.06689312e-01 -1.10026702e-01 -9.61613134e-02]\n",
      " [-1.84487607e-02  3.67946550e-03 -3.89079610e-03  1.02906302e-02]\n",
      " [-5.32284286e-03 -4.73137274e-02 -3.94571163e-02 -1.96331050e-02]\n",
      " [-3.08962632e-03 -4.96023335e-03 -4.87597613e-03 -1.81543268e-03]\n",
      " [ 2.45647296e-01  2.22980157e-01  2.09720701e-01  2.31041566e-01]\n",
      " [-1.69272155e-01 -1.58494845e-01 -1.59195349e-01 -1.44726336e-01]\n",
      " [ 2.80285299e-01  2.43148401e-01  2.33369634e-01  2.54019856e-01]\n",
      " [ 9.54361353e-03  1.61786284e-02  1.40711404e-02  1.57115869e-02]\n",
      " [ 1.21505424e-01  1.51184648e-01  1.47883698e-01  1.20375156e-01]\n",
      " [-2.83978939e-01 -3.26041669e-01 -2.99629807e-01 -2.93328613e-01]\n",
      " [ 4.44078371e-02  7.24886507e-02  6.52753413e-02  6.19411394e-02]\n",
      " [-4.50652000e-03 -5.84973395e-03 -5.87899564e-03 -2.46451609e-03]\n",
      " [-3.71811446e-03 -2.49143131e-03 -3.23643489e-03  5.04957512e-04]\n",
      " [ 3.71372439e-02  5.29242530e-02  4.81639653e-02  4.57300097e-02]\n",
      " [-2.08276045e-03 -1.86883938e-02 -1.35596879e-02 -1.49697531e-02]\n",
      " [-1.10383146e-04 -3.05888243e-03 -2.73473887e-03 -4.29609790e-04]\n",
      " [ 6.36848155e-03  5.75668178e-03  5.63753443e-03  6.61674887e-03]\n",
      " [-8.26231837e-01 -8.43300045e-01 -8.62029850e-01 -8.52148950e-01]\n",
      " [-7.95182645e-01 -8.16130936e-01 -8.23551595e-01 -8.17491591e-01]\n",
      " [-4.36503559e-01 -4.58811104e-01 -4.30504680e-01 -4.25903350e-01]\n",
      " [-1.50540769e-01 -1.38569012e-01 -1.39070749e-01 -1.28755108e-01]\n",
      " [-3.79830658e-01 -3.67219418e-01 -3.94582033e-01 -4.15952086e-01]\n",
      " [-3.31435800e-01 -3.82156909e-01 -3.50594789e-01 -3.47059667e-01]\n",
      " [ 2.17289194e-01  1.78842470e-01  1.71868980e-01  1.92033991e-01]\n",
      " [ 4.72742394e-02  5.95913082e-02  5.57957739e-02  5.04737198e-02]\n",
      " [ 1.23878196e-01  1.52657971e-01  1.49842024e-01  1.21229522e-01]\n",
      " [-3.86391222e-01 -3.95480305e-01 -3.76955479e-01 -3.76411587e-01]\n",
      " [-1.05269945e+00 -1.08213806e+00 -1.12099898e+00 -1.10459530e+00]\n",
      " [ 1.11330571e-02  3.49385142e-02  2.65952107e-02  4.07662541e-02]\n",
      " [ 1.01023817e+00  1.01072669e+00  1.02777886e+00  9.93270934e-01]\n",
      " [ 7.70175099e-01  7.92969048e-01  7.75199831e-01  7.61749804e-01]\n",
      " [-5.14387991e-03 -7.51575083e-03 -7.31612695e-03 -3.86132859e-03]\n",
      " [ 3.30829844e-02  4.80496585e-02  4.33317088e-02  4.19506654e-02]\n",
      " [-2.26753011e-01 -2.24042311e-01 -2.18946204e-01 -2.08352670e-01]\n",
      " [-8.28446001e-02 -1.11210443e-01 -9.97592360e-02 -8.86063874e-02]\n",
      " [-2.27795973e-01 -2.23971516e-01 -2.18691438e-01 -2.06256062e-01]\n",
      " [-2.88733691e-01 -2.86973715e-01 -2.77422279e-01 -2.71219194e-01]\n",
      " [-2.86408365e-02 -4.23113704e-02 -3.92888039e-02 -3.23241577e-02]\n",
      " [ 6.09422810e-02  7.04325363e-02  6.83805346e-02  5.78470156e-02]\n",
      " [-4.53040361e-01 -4.37304258e-01 -4.89993125e-01 -5.00592947e-01]\n",
      " [-6.29226565e-02 -9.47805196e-02 -8.37420151e-02 -7.69573152e-02]\n",
      " [ 6.95099607e-02  9.21252891e-02  8.62924159e-02  7.66548514e-02]\n",
      " [ 2.71362364e-02  4.05915007e-02  3.61773334e-02  3.60081121e-02]\n",
      " [ 1.03703275e-01  1.24106765e-01  1.21984251e-01  9.90248993e-02]\n",
      " [ 5.08640818e-02  6.96435273e-02  6.38887435e-02  5.93161732e-02]\n",
      " [ 6.71914890e-02  9.70815793e-02  8.94327685e-02  8.13479573e-02]\n",
      " [ 5.48578016e-02  8.38552341e-02  7.62719139e-02  7.10312873e-02]\n",
      " [-2.52834707e-01 -2.47251227e-01 -2.41751313e-01 -2.36035228e-01]\n",
      " [ 1.09308302e+00  1.06825900e+00  1.08527052e+00  1.08079016e+00]\n",
      " [-1.11932114e-01 -9.85533819e-02 -1.02668770e-01 -8.91567990e-02]\n",
      " [-1.94043353e-01 -1.87249869e-01 -1.85111970e-01 -1.70640707e-01]\n",
      " [ 1.59317292e-02  2.60929931e-02  2.29563788e-02  2.39672568e-02]\n",
      " [ 2.69299507e-01  2.41559193e-01  2.28359625e-01  2.50147551e-01]\n",
      " [ 1.45463059e-02 -8.22864287e-03 -8.91171023e-03  4.91998903e-03]\n",
      " [-2.00874638e-03 -9.77033563e-03 -8.37210193e-03 -6.12084009e-03]\n",
      " [-1.44018188e-01 -1.30102113e-01 -1.33035466e-01 -1.22049890e-01]\n",
      " [ 1.51331918e-02  3.79411206e-02  2.97447965e-02  4.26995084e-02]]\n",
      "\n",
      "\n",
      "1 day input [array([[-1.35552107, -1.3696643 , -1.35451061, -1.38659186],\n",
      "       [-1.4043607 , -1.43554755, -1.40393825, -1.42498902],\n",
      "       [-1.43226905, -1.43554755, -1.39687716, -1.41102642],\n",
      "       [-1.39738361, -1.34539153, -1.37216334, -1.29932558],\n",
      "       [-1.29272727, -1.32458629, -1.30861352, -1.31328819],\n",
      "       [-1.3031929 , -1.35232661, -1.3227357 , -1.35168535],\n",
      "       [-1.33807835, -1.34885907, -1.31920516, -1.35168535],\n",
      "       [-1.34156689, -1.35232661, -1.31214406, -1.30630689],\n",
      "       [-1.34156689, -1.32805383, -1.38628552, -1.40055446],\n",
      "       [-1.42529196, -1.42861247, -1.42512152, -1.45291423],\n",
      "       [-1.44971178, -1.41127478, -1.41806043, -1.36564795],\n",
      "       [-1.38342943, -1.34192399, -1.37922443, -1.40753577],\n",
      "       [-1.36947525, -1.37313184, -1.38628552, -1.37262926],\n",
      "       [-1.40784924, -1.44248263, -1.43218261, -1.47036748]])\n",
      " array([[-1.31365854, -1.30378106, -1.26977752, -1.30979754],\n",
      "       [-1.33110126, -1.31765121, -1.31214406, -1.28885363],\n",
      "       [-1.28575018, -1.32805383, -1.31920516, -1.355176  ],\n",
      "       [-1.34854398, -1.3696643 , -1.33332734, -1.36564795],\n",
      "       [-1.35552107, -1.3696643 , -1.35451061, -1.38659186],\n",
      "       [-1.4043607 , -1.43554755, -1.40393825, -1.42498902],\n",
      "       [-1.43226905, -1.43554755, -1.39687716, -1.41102642],\n",
      "       [-1.39738361, -1.34539153, -1.37216334, -1.29932558],\n",
      "       [-1.29272727, -1.32458629, -1.30861352, -1.31328819],\n",
      "       [-1.3031929 , -1.35232661, -1.3227357 , -1.35168535],\n",
      "       [-1.33807835, -1.34885907, -1.31920516, -1.35168535],\n",
      "       [-1.34156689, -1.35232661, -1.31214406, -1.30630689],\n",
      "       [-1.34156689, -1.32805383, -1.38628552, -1.40055446],\n",
      "       [-1.42529196, -1.42861247, -1.42512152, -1.45291423]])\n",
      " array([[-1.19853656, -1.24830043, -1.21681933, -1.22951256],\n",
      "       [-1.24388764, -1.28297582, -1.2450637 , -1.26790973],\n",
      "       [-1.26830746, -1.25870304, -1.22388043, -1.23300321],\n",
      "       [-1.27528455, -1.26217058, -1.26271643, -1.21554996],\n",
      "       [-1.18458239, -1.09919623, -1.14620842, -1.06894262],\n",
      "       [-1.03457497, -1.05758576, -1.0438226 , -1.06894262],\n",
      "       [-1.04852915, -1.07492346, -1.02970042, -1.05148936],\n",
      "       [-1.07294896, -1.07839099, -1.02970042, -1.05847066],\n",
      "       [-1.05550624, -0.96049465, -1.01204769, -0.91884462],\n",
      "       [-0.84619356, -0.86340355, -0.9273146 , -0.95026048],\n",
      "       [-1.01015516, -1.0263779 , -1.02616988, -1.03752676],\n",
      "       [-1.03108642, -1.08185853, -1.09325024, -1.13526499],\n",
      "       [-0.99271243, -0.92235172, -0.98027278, -1.02356415],\n",
      "       [-0.94038426, -0.96396219, -1.01557824, -1.04101741]])\n",
      " array([[0.32595743, 0.31902739, 0.3436818 , 0.34477105],\n",
      "       [0.26316362, 0.35023524, 0.30837634, 0.38316821],\n",
      "       [0.40270541, 0.48200174, 0.43194544, 0.49835969],\n",
      "       [0.49689611, 0.45079388, 0.47078144, 0.42505602],\n",
      "       [0.43759085, 0.41958603, 0.49196471, 0.45647188],\n",
      "       [0.43410231, 0.37450802, 0.37545671, 0.3412804 ],\n",
      "       [0.39223977, 0.39184571, 0.42841489, 0.37618691],\n",
      "       [0.39921686, 0.33983262, 0.39310943, 0.3622243 ],\n",
      "       [0.36433142, 0.31555985, 0.31190689, 0.25052347],\n",
      "       [0.214324  , 0.22887136, 0.25188761, 0.26448607],\n",
      "       [0.27362926, 0.21846874, 0.20599052, 0.22259826],\n",
      "       [0.58410973, 0.592963  , 0.35780398, 0.32731779],\n",
      "       [0.25269799, 0.36063786, 0.16009343, 0.35873365],\n",
      "       [0.36781996, 0.32943   , 0.31190689, 0.25750477]])\n",
      " array([[0.97831527, 0.97785987, 1.018016  , 1.02893865],\n",
      "       [1.03064343, 1.06108082, 1.09921854, 1.0463919 ],\n",
      "       [0.9887809 , 0.96052217, 0.9721189 , 0.95563498],\n",
      "       [0.95040691, 0.90504154, 0.91209963, 0.8613874 ],\n",
      "       [0.85272766, 0.87383369, 0.8944469 , 0.91374716],\n",
      "       [0.96436109, 0.96398971, 1.00389381, 1.01497604],\n",
      "       [1.00273508, 0.97785987, 1.03566872, 0.97657888],\n",
      "       [0.98529235, 0.9535871 , 0.98271054, 0.91374716],\n",
      "       [0.88761311, 0.89463893, 0.91916072, 0.95563498],\n",
      "       [0.95389545, 0.89810647, 0.96505781, 0.94516302],\n",
      "       [0.96087254, 0.95705464, 1.01095491, 1.00450409],\n",
      "       [1.02366634, 0.96745725, 1.03566872, 0.98705084],\n",
      "       [0.99226944, 0.96052217, 1.02860763, 0.96610693],\n",
      "       [0.96436109, 1.00906773, 1.00742436, 1.06384516]])\n",
      " array([[0.27362926, 0.21846874, 0.20599052, 0.22259826],\n",
      "       [0.58410973, 0.592963  , 0.35780398, 0.32731779],\n",
      "       [0.25269799, 0.36063786, 0.16009343, 0.35873365],\n",
      "       [0.36781996, 0.32943   , 0.31190689, 0.25750477],\n",
      "       [0.22478963, 0.2496766 , 0.17068507, 0.11438808],\n",
      "       [0.17595001, 0.16645565, 0.16009343, 0.13533199],\n",
      "       [0.1829271 , 0.28781953, 0.23070434, 0.30986454],\n",
      "       [0.3085147 , 0.27048183, 0.31190689, 0.29590193],\n",
      "       [0.30502616, 0.33983262, 0.35427343, 0.38316821],\n",
      "       [0.40270541, 0.35717032, 0.40017053, 0.36571495],\n",
      "       [0.38875123, 0.39184571, 0.43194544, 0.43901863],\n",
      "       [0.42014813, 0.37450802, 0.43194544, 0.41458407],\n",
      "       [0.43061376, 0.37104048, 0.4213538 , 0.36571495],\n",
      "       [0.36084287, 0.32943   , 0.35427343, 0.38665886]])\n",
      " array([[-1.48459722, -1.49796326, -1.44277425, -1.47036748],\n",
      "       [-1.46017741, -1.42861247, -1.42865207, -1.37961056],\n",
      "       [-1.4043607 , -1.4216774 , -1.37922443, -1.40404512],\n",
      "       [-1.41831487, -1.40780724, -1.38981607, -1.37961056],\n",
      "       [-1.36947525, -1.32458629, -1.34391897, -1.28536298],\n",
      "       [-1.21597929, -1.22056011, -1.23447206, -1.26441907],\n",
      "       [-1.21597929, -1.26217058, -1.27330806, -1.26790973],\n",
      "       [-1.31365854, -1.30378106, -1.26977752, -1.30979754],\n",
      "       [-1.33110126, -1.31765121, -1.31214406, -1.28885363],\n",
      "       [-1.28575018, -1.32805383, -1.31920516, -1.355176  ],\n",
      "       [-1.34854398, -1.3696643 , -1.33332734, -1.36564795],\n",
      "       [-1.35552107, -1.3696643 , -1.35451061, -1.38659186],\n",
      "       [-1.4043607 , -1.43554755, -1.40393825, -1.42498902],\n",
      "       [-1.43226905, -1.43554755, -1.39687716, -1.41102642]])\n",
      " array([[0.18641564, 0.13178026, 0.07536034, 0.04108441],\n",
      "       [0.02594259, 0.12484518, 0.08242143, 0.14580394],\n",
      "       [0.16548437, 0.38144309, 0.22717379, 0.35175235],\n",
      "       [0.42014813, 0.3641054 , 0.37192616, 0.32033649],\n",
      "       [0.32595743, 0.31902739, 0.3436818 , 0.34477105],\n",
      "       [0.26316362, 0.35023524, 0.30837634, 0.38316821],\n",
      "       [0.40270541, 0.48200174, 0.43194544, 0.49835969],\n",
      "       [0.49689611, 0.45079388, 0.47078144, 0.42505602],\n",
      "       [0.43759085, 0.41958603, 0.49196471, 0.45647188],\n",
      "       [0.43410231, 0.37450802, 0.37545671, 0.3412804 ],\n",
      "       [0.39223977, 0.39184571, 0.42841489, 0.37618691],\n",
      "       [0.39921686, 0.33983262, 0.39310943, 0.3622243 ],\n",
      "       [0.36433142, 0.31555985, 0.31190689, 0.25052347],\n",
      "       [0.214324  , 0.22887136, 0.25188761, 0.26448607]])\n",
      " array([[0.98529235, 0.96052217, 1.02154654, 0.99752279],\n",
      "       [0.97831527, 0.98132741, 1.02154654, 0.96610693],\n",
      "       [1.00622362, 1.04374312, 1.05332145, 1.08129841],\n",
      "       [1.04110907, 0.99519757, 1.018016  , 0.96610693],\n",
      "       [0.97831527, 0.96052217, 1.02507709, 0.99752279],\n",
      "       [0.99924653, 0.96745725, 1.00389381, 0.99752279],\n",
      "       [0.97831527, 0.97785987, 1.018016  , 1.02893865],\n",
      "       [1.03064343, 1.06108082, 1.09921854, 1.0463919 ],\n",
      "       [0.9887809 , 0.96052217, 0.9721189 , 0.95563498],\n",
      "       [0.95040691, 0.90504154, 0.91209963, 0.8613874 ],\n",
      "       [0.85272766, 0.87383369, 0.8944469 , 0.91374716],\n",
      "       [0.96436109, 0.96398971, 1.00389381, 1.01497604],\n",
      "       [1.00273508, 0.97785987, 1.03566872, 0.97657888],\n",
      "       [0.98529235, 0.9535871 , 0.98271054, 0.91374716]])\n",
      " array([[0.99226944, 0.96052217, 1.02860763, 0.96610693],\n",
      "       [0.96436109, 1.00906773, 1.00742436, 1.06384516],\n",
      "       [1.13181123, 1.34541905, 1.20160436, 1.39545701],\n",
      "       [1.3725208 , 1.32461381, 1.35694837, 1.29422813],\n",
      "       [1.35158954, 1.32461381, 1.38166219, 1.32564399],\n",
      "       [1.33414681, 1.26913318, 1.34988728, 1.30470008],\n",
      "       [1.26786447, 1.33501643, 1.32517346, 1.30470008],\n",
      "       [1.32716972, 1.27606826, 1.29692909, 1.25932162],\n",
      "       [1.27135301, 1.23099025, 1.29692909, 1.24186836],\n",
      "       [1.26088738, 1.20324993, 1.24750146, 1.23837771],\n",
      "       [1.19111649, 1.14083422, 1.22984873, 1.17903664],\n",
      "       [1.20507066, 1.16163946, 1.20160436, 1.14413013],\n",
      "       [1.14925395, 1.13043161, 1.19454327, 1.16507404],\n",
      "       [1.19460503, 1.1477693 , 1.13805454, 1.08827972]])\n",
      " array([[0.43759085, 0.41958603, 0.49196471, 0.45647188],\n",
      "       [0.43410231, 0.37450802, 0.37545671, 0.3412804 ],\n",
      "       [0.39223977, 0.39184571, 0.42841489, 0.37618691],\n",
      "       [0.39921686, 0.33983262, 0.39310943, 0.3622243 ],\n",
      "       [0.36433142, 0.31555985, 0.31190689, 0.25052347],\n",
      "       [0.214324  , 0.22887136, 0.25188761, 0.26448607],\n",
      "       [0.27362926, 0.21846874, 0.20599052, 0.22259826],\n",
      "       [0.58410973, 0.592963  , 0.35780398, 0.32731779],\n",
      "       [0.25269799, 0.36063786, 0.16009343, 0.35873365],\n",
      "       [0.36781996, 0.32943   , 0.31190689, 0.25750477],\n",
      "       [0.22478963, 0.2496766 , 0.17068507, 0.11438808],\n",
      "       [0.17595001, 0.16645565, 0.16009343, 0.13533199],\n",
      "       [0.1829271 , 0.28781953, 0.23070434, 0.30986454],\n",
      "       [0.3085147 , 0.27048183, 0.31190689, 0.29590193]])\n",
      " array([[1.27135301, 1.23099025, 1.29692909, 1.24186836],\n",
      "       [1.26088738, 1.20324993, 1.24750146, 1.23837771],\n",
      "       [1.19111649, 1.14083422, 1.22984873, 1.17903664],\n",
      "       [1.20507066, 1.16163946, 1.20160436, 1.14413013],\n",
      "       [1.14925395, 1.13043161, 1.19454327, 1.16507404],\n",
      "       [1.19460503, 1.1477693 , 1.13805454, 1.08827972],\n",
      "       [1.07948306, 1.05761328, 1.10274909, 1.11271427],\n",
      "       [1.09692578, 1.08535359, 1.095688  , 1.07780776],\n",
      "       [1.06552888, 1.03680804, 0.96152727, 0.92421912],\n",
      "       [0.86668184, 0.86689861, 0.90503854, 0.84742479],\n",
      "       [0.88412456, 0.92584678, 0.94387454, 0.99054149],\n",
      "       [0.96436109, 0.92931432, 0.9721189 , 0.93469107],\n",
      "       [0.92947564, 0.92584678, 0.9721189 , 0.90676586],\n",
      "       [0.94342982, 0.92931432, 0.96505781, 0.97657888]])\n",
      " array([[0.97831527, 0.96052217, 1.02507709, 0.99752279],\n",
      "       [0.99924653, 0.96745725, 1.00389381, 0.99752279],\n",
      "       [0.97831527, 0.97785987, 1.018016  , 1.02893865],\n",
      "       [1.03064343, 1.06108082, 1.09921854, 1.0463919 ],\n",
      "       [0.9887809 , 0.96052217, 0.9721189 , 0.95563498],\n",
      "       [0.95040691, 0.90504154, 0.91209963, 0.8613874 ],\n",
      "       [0.85272766, 0.87383369, 0.8944469 , 0.91374716],\n",
      "       [0.96436109, 0.96398971, 1.00389381, 1.01497604],\n",
      "       [1.00273508, 0.97785987, 1.03566872, 0.97657888],\n",
      "       [0.98529235, 0.9535871 , 0.98271054, 0.91374716],\n",
      "       [0.88761311, 0.89463893, 0.91916072, 0.95563498],\n",
      "       [0.95389545, 0.89810647, 0.96505781, 0.94516302],\n",
      "       [0.96087254, 0.95705464, 1.01095491, 1.00450409],\n",
      "       [1.02366634, 0.96745725, 1.03566872, 0.98705084]])\n",
      " array([[1.14925395, 1.13043161, 1.19454327, 1.16507404],\n",
      "       [1.19460503, 1.1477693 , 1.13805454, 1.08827972],\n",
      "       [1.07948306, 1.05761328, 1.10274909, 1.11271427],\n",
      "       [1.09692578, 1.08535359, 1.095688  , 1.07780776],\n",
      "       [1.06552888, 1.03680804, 0.96152727, 0.92421912],\n",
      "       [0.86668184, 0.86689861, 0.90503854, 0.84742479],\n",
      "       [0.88412456, 0.92584678, 0.94387454, 0.99054149],\n",
      "       [0.96436109, 0.92931432, 0.9721189 , 0.93469107],\n",
      "       [0.92947564, 0.92584678, 0.9721189 , 0.90676586],\n",
      "       [0.94342982, 0.92931432, 0.96505781, 0.97657888],\n",
      "       [0.97831527, 0.94318448, 1.02860763, 0.98006953],\n",
      "       [0.97831527, 0.9189117 , 0.91916072, 0.88582196],\n",
      "       [0.94342982, 0.93971694, 0.97918   , 0.95563498],\n",
      "       [0.97482672, 1.04374312, 1.01095491, 1.08478906]])\n",
      " array([[0.89807874, 0.99173003, 0.91209963, 1.0463919 ],\n",
      "       [1.04459761, 1.09575621, 1.09215745, 1.06733581],\n",
      "       [1.09692578, 1.07148344, 1.12040182, 1.11620492],\n",
      "       [1.14227686, 1.20671747, 1.18748218, 1.12318623],\n",
      "       [1.09692578, 1.04721066, 0.96152727, 1.09526102],\n",
      "       [1.03762052, 0.98132741, 1.04626036, 0.99403214],\n",
      "       [0.98529235, 0.96052217, 1.02154654, 0.99752279],\n",
      "       [0.97831527, 0.98132741, 1.02154654, 0.96610693],\n",
      "       [1.00622362, 1.04374312, 1.05332145, 1.08129841],\n",
      "       [1.04110907, 0.99519757, 1.018016  , 0.96610693],\n",
      "       [0.97831527, 0.96052217, 1.02507709, 0.99752279],\n",
      "       [0.99924653, 0.96745725, 1.00389381, 0.99752279],\n",
      "       [0.97831527, 0.97785987, 1.018016  , 1.02893865],\n",
      "       [1.03064343, 1.06108082, 1.09921854, 1.0463919 ]])\n",
      " array([[0.51782738, 0.49587189, 0.55551453, 0.5262849 ],\n",
      "       [0.51782738, 0.46466404, 0.46725089, 0.44599993],\n",
      "       [0.47596484, 0.52361221, 0.52727017, 0.58213532],\n",
      "       [0.5980639 , 0.75593735, 0.65083926, 0.74968657],\n",
      "       [0.78295677, 0.73859965, 0.70379744, 0.78110243],\n",
      "       [0.85272766, 0.98132741, 0.86973308, 0.96610693],\n",
      "       [0.957384  , 0.9535871 , 0.87326363, 0.84044349],\n",
      "       [0.81086512, 0.8044829 , 0.78499999, 0.76364917],\n",
      "       [0.89459019, 0.96052217, 0.91916072, 0.96261628],\n",
      "       [0.97482672, 0.94665202, 0.98271054, 0.91723782],\n",
      "       [0.95389545, 1.07495098, 1.01095491, 1.07780776],\n",
      "       [1.22949048, 1.25179548, 1.056852  , 1.12318623],\n",
      "       [1.20158212, 1.14083422, 1.04272982, 1.13365818],\n",
      "       [1.34461245, 1.32461381, 1.27927637, 1.33262529]])\n",
      " array([[1.07948306, 1.05761328, 1.10274909, 1.11271427],\n",
      "       [1.09692578, 1.08535359, 1.095688  , 1.07780776],\n",
      "       [1.06552888, 1.03680804, 0.96152727, 0.92421912],\n",
      "       [0.86668184, 0.86689861, 0.90503854, 0.84742479],\n",
      "       [0.88412456, 0.92584678, 0.94387454, 0.99054149],\n",
      "       [0.96436109, 0.92931432, 0.9721189 , 0.93469107],\n",
      "       [0.92947564, 0.92584678, 0.9721189 , 0.90676586],\n",
      "       [0.94342982, 0.92931432, 0.96505781, 0.97657888],\n",
      "       [0.97831527, 0.94318448, 1.02860763, 0.98006953],\n",
      "       [0.97831527, 0.9189117 , 0.91916072, 0.88582196],\n",
      "       [0.94342982, 0.93971694, 0.97918   , 0.95563498],\n",
      "       [0.97482672, 1.04374312, 1.01095491, 1.08478906],\n",
      "       [1.12483414, 1.07841851, 1.12040182, 1.09177037],\n",
      "       [1.11087996, 1.04721066, 1.10274909, 1.04988255]])\n",
      " array([[1.03064343, 1.06108082, 1.09921854, 1.0463919 ],\n",
      "       [0.9887809 , 0.96052217, 0.9721189 , 0.95563498],\n",
      "       [0.95040691, 0.90504154, 0.91209963, 0.8613874 ],\n",
      "       [0.85272766, 0.87383369, 0.8944469 , 0.91374716],\n",
      "       [0.96436109, 0.96398971, 1.00389381, 1.01497604],\n",
      "       [1.00273508, 0.97785987, 1.03566872, 0.97657888],\n",
      "       [0.98529235, 0.9535871 , 0.98271054, 0.91374716],\n",
      "       [0.88761311, 0.89463893, 0.91916072, 0.95563498],\n",
      "       [0.95389545, 0.89810647, 0.96505781, 0.94516302],\n",
      "       [0.96087254, 0.95705464, 1.01095491, 1.00450409],\n",
      "       [1.02366634, 0.96745725, 1.03566872, 0.98705084],\n",
      "       [0.99226944, 0.96052217, 1.02860763, 0.96610693],\n",
      "       [0.96436109, 1.00906773, 1.00742436, 1.06384516],\n",
      "       [1.13181123, 1.34541905, 1.20160436, 1.39545701]])\n",
      " array([[0.22478963, 0.33636508, 0.28719307, 0.3622243 ],\n",
      "       [0.25269799, 0.33289754, 0.30837634, 0.33778975],\n",
      "       [0.32246888, 0.30862477, 0.29425416, 0.28193933],\n",
      "       [0.31898034, 0.28781953, 0.21305161, 0.16674785],\n",
      "       [0.20734691, 0.21846874, 0.16362397, 0.26448607],\n",
      "       [0.27014071, 0.21846874, 0.25188761, 0.20165436],\n",
      "       [0.20385836, 0.16298811, 0.21658216, 0.1842011 ],\n",
      "       [0.14804165, 0.20806612, 0.19539888, 0.15976654],\n",
      "       [0.03989677, 0.12137764, 0.06123815, 0.14580394],\n",
      "       [0.12362184, 0.08323471, 0.10713525, 0.08995352],\n",
      "       [0.1131562 , 0.15952057, 0.15303234, 0.1772198 ],\n",
      "       [0.17246146, 0.11444256, 0.15303234, 0.10391613],\n",
      "       [0.12013329, 0.09016978, 0.13891016, 0.10042548],\n",
      "       [0.10617912, 0.08670225, 0.12478797, 0.07250027]])\n",
      " array([[0.10617912, 0.08670225, 0.12478797, 0.07250027],\n",
      "       [0.07827076, 0.11444256, 0.12831852, 0.12486003],\n",
      "       [0.10269057, 0.10750748, 0.13891016, 0.15627589],\n",
      "       [0.18641564, 0.13178026, 0.07536034, 0.04108441],\n",
      "       [0.02594259, 0.12484518, 0.08242143, 0.14580394],\n",
      "       [0.16548437, 0.38144309, 0.22717379, 0.35175235],\n",
      "       [0.42014813, 0.3641054 , 0.37192616, 0.32033649],\n",
      "       [0.32595743, 0.31902739, 0.3436818 , 0.34477105],\n",
      "       [0.26316362, 0.35023524, 0.30837634, 0.38316821],\n",
      "       [0.40270541, 0.48200174, 0.43194544, 0.49835969],\n",
      "       [0.49689611, 0.45079388, 0.47078144, 0.42505602],\n",
      "       [0.43759085, 0.41958603, 0.49196471, 0.45647188],\n",
      "       [0.43410231, 0.37450802, 0.37545671, 0.3412804 ],\n",
      "       [0.39223977, 0.39184571, 0.42841489, 0.37618691]])\n",
      " array([[-1.48459722, -1.50489834, -1.47807971, -1.49829269],\n",
      "       [-1.4985514 , -1.50836588, -1.47101861, -1.48083944],\n",
      "       [-1.43924614, -1.45982033, -1.41452989, -1.44942358],\n",
      "       [-1.44622323, -1.48756064, -1.45689643, -1.47734879],\n",
      "       [-1.47064304, -1.46675541, -1.42865207, -1.43895163],\n",
      "       [-1.42529196, -1.46675541, -1.4392437 , -1.43546097],\n",
      "       [-1.50203994, -1.42861247, -1.46042698, -1.38310121],\n",
      "       [-1.38342943, -1.41127478, -1.36510225, -1.37611991],\n",
      "       [-1.36947525, -1.36619676, -1.4392437 , -1.45989553],\n",
      "       [-1.44273469, -1.42861247, -1.40393825, -1.41102642],\n",
      "       [-1.39389506, -1.33498891, -1.35804116, -1.29234428],\n",
      "       [-1.1462084 , -1.19628733, -1.23094152, -1.22602191],\n",
      "       [-1.19853656, -1.24830043, -1.21681933, -1.22951256],\n",
      "       [-1.24388764, -1.28297582, -1.2450637 , -1.26790973]])\n",
      " array([[-1.22644492, -1.2725732 , -1.23447206, -1.27140038],\n",
      "       [-1.27528455, -1.23443027, -1.23447206, -1.20856866],\n",
      "       [-1.17411675, -1.22056011, -1.29449134, -1.31328819],\n",
      "       [-1.34505544, -1.39046954, -1.38628552, -1.42847967],\n",
      "       [-1.43226905, -1.46675541, -1.47807971, -1.49480204],\n",
      "       [-1.47064304, -1.5014308 , -1.47101861, -1.48083944],\n",
      "       [-1.48459722, -1.49796326, -1.44277425, -1.47036748],\n",
      "       [-1.46017741, -1.42861247, -1.42865207, -1.37961056],\n",
      "       [-1.4043607 , -1.4216774 , -1.37922443, -1.40404512],\n",
      "       [-1.41831487, -1.40780724, -1.38981607, -1.37961056],\n",
      "       [-1.36947525, -1.32458629, -1.34391897, -1.28536298],\n",
      "       [-1.21597929, -1.22056011, -1.23447206, -1.26441907],\n",
      "       [-1.21597929, -1.26217058, -1.27330806, -1.26790973],\n",
      "       [-1.31365854, -1.30378106, -1.26977752, -1.30979754]])\n",
      " array([[0.27014071, 0.21846874, 0.25188761, 0.20165436],\n",
      "       [0.20385836, 0.16298811, 0.21658216, 0.1842011 ],\n",
      "       [0.14804165, 0.20806612, 0.19539888, 0.15976654],\n",
      "       [0.03989677, 0.12137764, 0.06123815, 0.14580394],\n",
      "       [0.12362184, 0.08323471, 0.10713525, 0.08995352],\n",
      "       [0.1131562 , 0.15952057, 0.15303234, 0.1772198 ],\n",
      "       [0.17246146, 0.11444256, 0.15303234, 0.10391613],\n",
      "       [0.12013329, 0.09016978, 0.13891016, 0.10042548],\n",
      "       [0.10617912, 0.08670225, 0.12478797, 0.07250027],\n",
      "       [0.07827076, 0.11444256, 0.12831852, 0.12486003],\n",
      "       [0.10269057, 0.10750748, 0.13891016, 0.15627589],\n",
      "       [0.18641564, 0.13178026, 0.07536034, 0.04108441],\n",
      "       [0.02594259, 0.12484518, 0.08242143, 0.14580394],\n",
      "       [0.16548437, 0.38144309, 0.22717379, 0.35175235]])\n",
      " array([[0.49689611, 0.45079388, 0.47078144, 0.42505602],\n",
      "       [0.43759085, 0.41958603, 0.49196471, 0.45647188],\n",
      "       [0.43410231, 0.37450802, 0.37545671, 0.3412804 ],\n",
      "       [0.39223977, 0.39184571, 0.42841489, 0.37618691],\n",
      "       [0.39921686, 0.33983262, 0.39310943, 0.3622243 ],\n",
      "       [0.36433142, 0.31555985, 0.31190689, 0.25052347],\n",
      "       [0.214324  , 0.22887136, 0.25188761, 0.26448607],\n",
      "       [0.27362926, 0.21846874, 0.20599052, 0.22259826],\n",
      "       [0.58410973, 0.592963  , 0.35780398, 0.32731779],\n",
      "       [0.25269799, 0.36063786, 0.16009343, 0.35873365],\n",
      "       [0.36781996, 0.32943   , 0.31190689, 0.25750477],\n",
      "       [0.22478963, 0.2496766 , 0.17068507, 0.11438808],\n",
      "       [0.17595001, 0.16645565, 0.16009343, 0.13533199],\n",
      "       [0.1829271 , 0.28781953, 0.23070434, 0.30986454]])\n",
      " array([[0.16548437, 0.38144309, 0.22717379, 0.35175235],\n",
      "       [0.42014813, 0.3641054 , 0.37192616, 0.32033649],\n",
      "       [0.32595743, 0.31902739, 0.3436818 , 0.34477105],\n",
      "       [0.26316362, 0.35023524, 0.30837634, 0.38316821],\n",
      "       [0.40270541, 0.48200174, 0.43194544, 0.49835969],\n",
      "       [0.49689611, 0.45079388, 0.47078144, 0.42505602],\n",
      "       [0.43759085, 0.41958603, 0.49196471, 0.45647188],\n",
      "       [0.43410231, 0.37450802, 0.37545671, 0.3412804 ],\n",
      "       [0.39223977, 0.39184571, 0.42841489, 0.37618691],\n",
      "       [0.39921686, 0.33983262, 0.39310943, 0.3622243 ],\n",
      "       [0.36433142, 0.31555985, 0.31190689, 0.25052347],\n",
      "       [0.214324  , 0.22887136, 0.25188761, 0.26448607],\n",
      "       [0.27362926, 0.21846874, 0.20599052, 0.22259826],\n",
      "       [0.58410973, 0.592963  , 0.35780398, 0.32731779]])\n",
      " array([[0.83877348, 0.86689861, 0.8167749 , 0.74968657],\n",
      "       [0.71318588, 0.83569076, 0.72145017, 0.87185935],\n",
      "       [0.90505583, 0.90504154, 0.89091636, 0.92072847],\n",
      "       [0.89459019, 0.87730123, 0.91563018, 0.90676586],\n",
      "       [0.91552146, 0.98479495, 0.95799672, 1.02893865],\n",
      "       [1.05506325, 1.00906773, 1.03919927, 0.96959758],\n",
      "       [1.00273508, 0.98479495, 1.04626036, 1.02195735],\n",
      "       [1.08646015, 1.05761328, 1.12393236, 1.08478906],\n",
      "       [1.0829716 , 1.0333405 , 1.12393236, 1.06384516],\n",
      "       [1.06204034, 1.04374312, 1.08509636, 1.06035451],\n",
      "       [1.01320071, 1.0333405 , 1.02507709, 1.08129841],\n",
      "       [1.09692578, 1.1477693 , 1.14158509, 1.13365818],\n",
      "       [1.13529977, 1.09575621, 1.15923782, 1.13016753],\n",
      "       [1.10041433, 1.04721066, 1.08509636, 1.07780776]])\n",
      " array([[1.02366634, 0.96745725, 1.03566872, 0.98705084],\n",
      "       [0.99226944, 0.96052217, 1.02860763, 0.96610693],\n",
      "       [0.96436109, 1.00906773, 1.00742436, 1.06384516],\n",
      "       [1.13181123, 1.34541905, 1.20160436, 1.39545701],\n",
      "       [1.3725208 , 1.32461381, 1.35694837, 1.29422813],\n",
      "       [1.35158954, 1.32461381, 1.38166219, 1.32564399],\n",
      "       [1.33414681, 1.26913318, 1.34988728, 1.30470008],\n",
      "       [1.26786447, 1.33501643, 1.32517346, 1.30470008],\n",
      "       [1.32716972, 1.27606826, 1.29692909, 1.25932162],\n",
      "       [1.27135301, 1.23099025, 1.29692909, 1.24186836],\n",
      "       [1.26088738, 1.20324993, 1.24750146, 1.23837771],\n",
      "       [1.19111649, 1.14083422, 1.22984873, 1.17903664],\n",
      "       [1.20507066, 1.16163946, 1.20160436, 1.14413013],\n",
      "       [1.14925395, 1.13043161, 1.19454327, 1.16507404]])\n",
      " array([[-1.36947525, -1.37313184, -1.38628552, -1.37262926],\n",
      "       [-1.40784924, -1.44248263, -1.43218261, -1.47036748],\n",
      "       [-1.44971178, -1.49796326, -1.47101861, -1.46338618],\n",
      "       [-1.46017741, -1.49796326, -1.47807971, -1.5192366 ],\n",
      "       [-1.50552849, -1.54997635, -1.50279352, -1.53668985],\n",
      "       [-1.52297121, -1.51183342, -1.49220189, -1.46687683],\n",
      "       [-1.46017741, -1.49102818, -1.47101861, -1.48433009],\n",
      "       [-1.48459722, -1.28644336, -1.46042698, -1.25743777],\n",
      "       [-1.23691056, -1.1581444 , -1.24153315, -1.15271824],\n",
      "       [-1.19853656, -1.17201456, -1.17092224, -1.21205931],\n",
      "       [-1.29970436, -1.34192399, -1.39687716, -1.39706381],\n",
      "       [-1.37994088, -1.36619676, -1.40393825, -1.43197032],\n",
      "       [-1.42180342, -1.4216774 , -1.44277425, -1.48083944],\n",
      "       [-1.48459722, -1.50489834, -1.47807971, -1.49829269]])\n",
      " array([[0.10966766, 0.14218288, 0.11419634, 0.1842011 ],\n",
      "       [0.18641564, 0.20113105, 0.20245997, 0.1772198 ],\n",
      "       [0.16199583, 0.17685827, 0.21305161, 0.21212631],\n",
      "       [0.22478963, 0.33636508, 0.28719307, 0.3622243 ],\n",
      "       [0.25269799, 0.33289754, 0.30837634, 0.33778975],\n",
      "       [0.32246888, 0.30862477, 0.29425416, 0.28193933],\n",
      "       [0.31898034, 0.28781953, 0.21305161, 0.16674785],\n",
      "       [0.20734691, 0.21846874, 0.16362397, 0.26448607],\n",
      "       [0.27014071, 0.21846874, 0.25188761, 0.20165436],\n",
      "       [0.20385836, 0.16298811, 0.21658216, 0.1842011 ],\n",
      "       [0.14804165, 0.20806612, 0.19539888, 0.15976654],\n",
      "       [0.03989677, 0.12137764, 0.06123815, 0.14580394],\n",
      "       [0.12362184, 0.08323471, 0.10713525, 0.08995352],\n",
      "       [0.1131562 , 0.15952057, 0.15303234, 0.1772198 ]])\n",
      " array([[-1.13923131, -1.14774178, -1.11443351, -1.12130238],\n",
      "       [-1.14969694, -1.1581444 , -1.1285557 , -1.15271824],\n",
      "       [-1.14969694, -1.16854702, -1.13208624, -1.14573694],\n",
      "       [-1.16713966, -1.19975487, -1.17445279, -1.21554996],\n",
      "       [-1.19155948, -1.16854702, -1.16033061, -1.15620889],\n",
      "       [-1.17411675, -1.18935226, -1.16033061, -1.14922759],\n",
      "       [-1.14969694, -1.17894964, -1.13561679, -1.15620889],\n",
      "       [-1.16365112, -1.18241718, -1.18857497, -1.21904061],\n",
      "       [-1.22644492, -1.2725732 , -1.23447206, -1.27140038],\n",
      "       [-1.27528455, -1.23443027, -1.23447206, -1.20856866],\n",
      "       [-1.17411675, -1.22056011, -1.29449134, -1.31328819],\n",
      "       [-1.34505544, -1.39046954, -1.38628552, -1.42847967],\n",
      "       [-1.43226905, -1.46675541, -1.47807971, -1.49480204],\n",
      "       [-1.47064304, -1.5014308 , -1.47101861, -1.48083944]])\n",
      " array([[ 0.06780512,  0.01388392,  0.04358543,  0.01664985],\n",
      "       [ 0.01198841, -0.04506425, -0.07998367, -0.06014447],\n",
      "       [-0.00894286, -0.0138564 , -0.02702549, -0.07061642],\n",
      "       [-0.09615647, -0.0138564 , -0.06233094,  0.0271218 ],\n",
      "       [ 0.01547696,  0.06242947,  0.07182979,  0.08646287],\n",
      "       [ 0.10966766,  0.14218288,  0.11419634,  0.1842011 ],\n",
      "       [ 0.18641564,  0.20113105,  0.20245997,  0.1772198 ],\n",
      "       [ 0.16199583,  0.17685827,  0.21305161,  0.21212631],\n",
      "       [ 0.22478963,  0.33636508,  0.28719307,  0.3622243 ],\n",
      "       [ 0.25269799,  0.33289754,  0.30837634,  0.33778975],\n",
      "       [ 0.32246888,  0.30862477,  0.29425416,  0.28193933],\n",
      "       [ 0.31898034,  0.28781953,  0.21305161,  0.16674785],\n",
      "       [ 0.20734691,  0.21846874,  0.16362397,  0.26448607],\n",
      "       [ 0.27014071,  0.21846874,  0.25188761,  0.20165436]])\n",
      " array([[-0.17290445, -0.08667473, -0.11528912, -0.09854163],\n",
      "       [ 0.12362184,  0.11444256,  0.07536034,  0.03410311],\n",
      "       [ 0.06082804,  0.01388392,  0.05417706,  0.05155636],\n",
      "       [ 0.06780512,  0.01388392,  0.04358543,  0.01664985],\n",
      "       [ 0.01198841, -0.04506425, -0.07998367, -0.06014447],\n",
      "       [-0.00894286, -0.0138564 , -0.02702549, -0.07061642],\n",
      "       [-0.09615647, -0.0138564 , -0.06233094,  0.0271218 ],\n",
      "       [ 0.01547696,  0.06242947,  0.07182979,  0.08646287],\n",
      "       [ 0.10966766,  0.14218288,  0.11419634,  0.1842011 ],\n",
      "       [ 0.18641564,  0.20113105,  0.20245997,  0.1772198 ],\n",
      "       [ 0.16199583,  0.17685827,  0.21305161,  0.21212631],\n",
      "       [ 0.22478963,  0.33636508,  0.28719307,  0.3622243 ],\n",
      "       [ 0.25269799,  0.33289754,  0.30837634,  0.33778975],\n",
      "       [ 0.32246888,  0.30862477,  0.29425416,  0.28193933]])\n",
      " array([[0.99924653, 0.96745725, 1.00389381, 0.99752279],\n",
      "       [0.97831527, 0.97785987, 1.018016  , 1.02893865],\n",
      "       [1.03064343, 1.06108082, 1.09921854, 1.0463919 ],\n",
      "       [0.9887809 , 0.96052217, 0.9721189 , 0.95563498],\n",
      "       [0.95040691, 0.90504154, 0.91209963, 0.8613874 ],\n",
      "       [0.85272766, 0.87383369, 0.8944469 , 0.91374716],\n",
      "       [0.96436109, 0.96398971, 1.00389381, 1.01497604],\n",
      "       [1.00273508, 0.97785987, 1.03566872, 0.97657888],\n",
      "       [0.98529235, 0.9535871 , 0.98271054, 0.91374716],\n",
      "       [0.88761311, 0.89463893, 0.91916072, 0.95563498],\n",
      "       [0.95389545, 0.89810647, 0.96505781, 0.94516302],\n",
      "       [0.96087254, 0.95705464, 1.01095491, 1.00450409],\n",
      "       [1.02366634, 0.96745725, 1.03566872, 0.98705084],\n",
      "       [0.99226944, 0.96052217, 1.02860763, 0.96610693]])\n",
      " array([[1.04459761, 1.09575621, 1.09215745, 1.06733581],\n",
      "       [1.09692578, 1.07148344, 1.12040182, 1.11620492],\n",
      "       [1.14227686, 1.20671747, 1.18748218, 1.12318623],\n",
      "       [1.09692578, 1.04721066, 0.96152727, 1.09526102],\n",
      "       [1.03762052, 0.98132741, 1.04626036, 0.99403214],\n",
      "       [0.98529235, 0.96052217, 1.02154654, 0.99752279],\n",
      "       [0.97831527, 0.98132741, 1.02154654, 0.96610693],\n",
      "       [1.00622362, 1.04374312, 1.05332145, 1.08129841],\n",
      "       [1.04110907, 0.99519757, 1.018016  , 0.96610693],\n",
      "       [0.97831527, 0.96052217, 1.02507709, 0.99752279],\n",
      "       [0.99924653, 0.96745725, 1.00389381, 0.99752279],\n",
      "       [0.97831527, 0.97785987, 1.018016  , 1.02893865],\n",
      "       [1.03064343, 1.06108082, 1.09921854, 1.0463919 ],\n",
      "       [0.9887809 , 0.96052217, 0.9721189 , 0.95563498]])\n",
      " array([[-1.46017741, -1.49796326, -1.47807971, -1.5192366 ],\n",
      "       [-1.50552849, -1.54997635, -1.50279352, -1.53668985],\n",
      "       [-1.52297121, -1.51183342, -1.49220189, -1.46687683],\n",
      "       [-1.46017741, -1.49102818, -1.47101861, -1.48433009],\n",
      "       [-1.48459722, -1.28644336, -1.46042698, -1.25743777],\n",
      "       [-1.23691056, -1.1581444 , -1.24153315, -1.15271824],\n",
      "       [-1.19853656, -1.17201456, -1.17092224, -1.21205931],\n",
      "       [-1.29970436, -1.34192399, -1.39687716, -1.39706381],\n",
      "       [-1.37994088, -1.36619676, -1.40393825, -1.43197032],\n",
      "       [-1.42180342, -1.4216774 , -1.44277425, -1.48083944],\n",
      "       [-1.48459722, -1.50489834, -1.47807971, -1.49829269],\n",
      "       [-1.4985514 , -1.50836588, -1.47101861, -1.48083944],\n",
      "       [-1.43924614, -1.45982033, -1.41452989, -1.44942358],\n",
      "       [-1.44622323, -1.48756064, -1.45689643, -1.47734879]])\n",
      " array([[0.30502616, 0.33983262, 0.35427343, 0.38316821],\n",
      "       [0.40270541, 0.35717032, 0.40017053, 0.36571495],\n",
      "       [0.38875123, 0.39184571, 0.43194544, 0.43901863],\n",
      "       [0.42014813, 0.37450802, 0.43194544, 0.41458407],\n",
      "       [0.43061376, 0.37104048, 0.4213538 , 0.36571495],\n",
      "       [0.36084287, 0.32943   , 0.35427343, 0.38665886],\n",
      "       [0.36781996, 0.42305357, 0.4213538 , 0.48090644],\n",
      "       [0.51085029, 0.50627451, 0.56257562, 0.56468206],\n",
      "       [0.51782738, 0.49587189, 0.55551453, 0.5262849 ],\n",
      "       [0.51782738, 0.46466404, 0.46725089, 0.44599993],\n",
      "       [0.47596484, 0.52361221, 0.52727017, 0.58213532],\n",
      "       [0.5980639 , 0.75593735, 0.65083926, 0.74968657],\n",
      "       [0.78295677, 0.73859965, 0.70379744, 0.78110243],\n",
      "       [0.85272766, 0.98132741, 0.86973308, 0.96610693]])\n",
      " array([[-1.47064304, -1.5014308 , -1.47101861, -1.48083944],\n",
      "       [-1.48459722, -1.49796326, -1.44277425, -1.47036748],\n",
      "       [-1.46017741, -1.42861247, -1.42865207, -1.37961056],\n",
      "       [-1.4043607 , -1.4216774 , -1.37922443, -1.40404512],\n",
      "       [-1.41831487, -1.40780724, -1.38981607, -1.37961056],\n",
      "       [-1.36947525, -1.32458629, -1.34391897, -1.28536298],\n",
      "       [-1.21597929, -1.22056011, -1.23447206, -1.26441907],\n",
      "       [-1.21597929, -1.26217058, -1.27330806, -1.26790973],\n",
      "       [-1.31365854, -1.30378106, -1.26977752, -1.30979754],\n",
      "       [-1.33110126, -1.31765121, -1.31214406, -1.28885363],\n",
      "       [-1.28575018, -1.32805383, -1.31920516, -1.355176  ],\n",
      "       [-1.34854398, -1.3696643 , -1.33332734, -1.36564795],\n",
      "       [-1.35552107, -1.3696643 , -1.35451061, -1.38659186],\n",
      "       [-1.4043607 , -1.43554755, -1.40393825, -1.42498902]])\n",
      " array([[-0.13453046, -0.16296059, -0.16118622, -0.21722377],\n",
      "       [-0.25662952, -0.26351923, -0.23885822, -0.24165832],\n",
      "       [-0.24965243, -0.23924646, -0.21061385, -0.20675181],\n",
      "       [-0.17290445, -0.08667473, -0.11528912, -0.09854163],\n",
      "       [ 0.12362184,  0.11444256,  0.07536034,  0.03410311],\n",
      "       [ 0.06082804,  0.01388392,  0.05417706,  0.05155636],\n",
      "       [ 0.06780512,  0.01388392,  0.04358543,  0.01664985],\n",
      "       [ 0.01198841, -0.04506425, -0.07998367, -0.06014447],\n",
      "       [-0.00894286, -0.0138564 , -0.02702549, -0.07061642],\n",
      "       [-0.09615647, -0.0138564 , -0.06233094,  0.0271218 ],\n",
      "       [ 0.01547696,  0.06242947,  0.07182979,  0.08646287],\n",
      "       [ 0.10966766,  0.14218288,  0.11419634,  0.1842011 ],\n",
      "       [ 0.18641564,  0.20113105,  0.20245997,  0.1772198 ],\n",
      "       [ 0.16199583,  0.17685827,  0.21305161,  0.21212631]])\n",
      " array([[1.22949048, 1.25179548, 1.056852  , 1.12318623],\n",
      "       [1.20158212, 1.14083422, 1.04272982, 1.13365818],\n",
      "       [1.34461245, 1.32461381, 1.27927637, 1.33262529],\n",
      "       [1.30623846, 1.35235413, 1.28633746, 1.27328422],\n",
      "       [1.14925395, 1.11656145, 0.96505781, 1.01148539],\n",
      "       [0.81435367, 0.81141798, 0.62259489, 0.66940159],\n",
      "       [0.65736916, 0.72819703, 0.72145017, 0.77761177],\n",
      "       [0.83179639, 0.98479495, 0.89797745, 1.0463919 ],\n",
      "       [1.03762052, 0.98132741, 1.018016  , 0.97657888],\n",
      "       [0.99226944, 1.07148344, 1.03919927, 1.13714883],\n",
      "       [1.12832268, 1.09922375, 1.06038254, 1.01148539],\n",
      "       [0.99226944, 0.98132741, 0.98624109, 1.00101344],\n",
      "       [0.97831527, 0.94318448, 0.96858836, 0.96959758],\n",
      "       [0.90854437, 0.92237924, 0.83442763, 0.80553698]])\n",
      " array([[0.85272766, 0.87383369, 0.8944469 , 0.91374716],\n",
      "       [0.96436109, 0.96398971, 1.00389381, 1.01497604],\n",
      "       [1.00273508, 0.97785987, 1.03566872, 0.97657888],\n",
      "       [0.98529235, 0.9535871 , 0.98271054, 0.91374716],\n",
      "       [0.88761311, 0.89463893, 0.91916072, 0.95563498],\n",
      "       [0.95389545, 0.89810647, 0.96505781, 0.94516302],\n",
      "       [0.96087254, 0.95705464, 1.01095491, 1.00450409],\n",
      "       [1.02366634, 0.96745725, 1.03566872, 0.98705084],\n",
      "       [0.99226944, 0.96052217, 1.02860763, 0.96610693],\n",
      "       [0.96436109, 1.00906773, 1.00742436, 1.06384516],\n",
      "       [1.13181123, 1.34541905, 1.20160436, 1.39545701],\n",
      "       [1.3725208 , 1.32461381, 1.35694837, 1.29422813],\n",
      "       [1.35158954, 1.32461381, 1.38166219, 1.32564399],\n",
      "       [1.33414681, 1.26913318, 1.34988728, 1.30470008]])\n",
      " array([[0.20734691, 0.21846874, 0.16362397, 0.26448607],\n",
      "       [0.27014071, 0.21846874, 0.25188761, 0.20165436],\n",
      "       [0.20385836, 0.16298811, 0.21658216, 0.1842011 ],\n",
      "       [0.14804165, 0.20806612, 0.19539888, 0.15976654],\n",
      "       [0.03989677, 0.12137764, 0.06123815, 0.14580394],\n",
      "       [0.12362184, 0.08323471, 0.10713525, 0.08995352],\n",
      "       [0.1131562 , 0.15952057, 0.15303234, 0.1772198 ],\n",
      "       [0.17246146, 0.11444256, 0.15303234, 0.10391613],\n",
      "       [0.12013329, 0.09016978, 0.13891016, 0.10042548],\n",
      "       [0.10617912, 0.08670225, 0.12478797, 0.07250027],\n",
      "       [0.07827076, 0.11444256, 0.12831852, 0.12486003],\n",
      "       [0.10269057, 0.10750748, 0.13891016, 0.15627589],\n",
      "       [0.18641564, 0.13178026, 0.07536034, 0.04108441],\n",
      "       [0.02594259, 0.12484518, 0.08242143, 0.14580394]])\n",
      " array([[0.43061376, 0.37104048, 0.4213538 , 0.36571495],\n",
      "       [0.36084287, 0.32943   , 0.35427343, 0.38665886],\n",
      "       [0.36781996, 0.42305357, 0.4213538 , 0.48090644],\n",
      "       [0.51085029, 0.50627451, 0.56257562, 0.56468206],\n",
      "       [0.51782738, 0.49587189, 0.55551453, 0.5262849 ],\n",
      "       [0.51782738, 0.46466404, 0.46725089, 0.44599993],\n",
      "       [0.47596484, 0.52361221, 0.52727017, 0.58213532],\n",
      "       [0.5980639 , 0.75593735, 0.65083926, 0.74968657],\n",
      "       [0.78295677, 0.73859965, 0.70379744, 0.78110243],\n",
      "       [0.85272766, 0.98132741, 0.86973308, 0.96610693],\n",
      "       [0.957384  , 0.9535871 , 0.87326363, 0.84044349],\n",
      "       [0.81086512, 0.8044829 , 0.78499999, 0.76364917],\n",
      "       [0.89459019, 0.96052217, 0.91916072, 0.96261628],\n",
      "       [0.97482672, 0.94665202, 0.98271054, 0.91723782]])\n",
      " array([[1.05506325, 1.00906773, 1.03919927, 0.96959758],\n",
      "       [1.00273508, 0.98479495, 1.04626036, 1.02195735],\n",
      "       [1.08646015, 1.05761328, 1.12393236, 1.08478906],\n",
      "       [1.0829716 , 1.0333405 , 1.12393236, 1.06384516],\n",
      "       [1.06204034, 1.04374312, 1.08509636, 1.06035451],\n",
      "       [1.01320071, 1.0333405 , 1.02507709, 1.08129841],\n",
      "       [1.09692578, 1.1477693 , 1.14158509, 1.13365818],\n",
      "       [1.13529977, 1.09575621, 1.15923782, 1.13016753],\n",
      "       [1.10041433, 1.04721066, 1.08509636, 1.07780776],\n",
      "       [1.07250597, 1.13389915, 1.14158509, 1.16158339],\n",
      "       [1.17716231, 1.12002899, 1.14864618, 1.11620492],\n",
      "       [1.14227686, 1.09575621, 1.11687127, 1.06035451],\n",
      "       [1.03413198, 1.02293788, 1.05332145, 1.08478906],\n",
      "       [1.06552888, 1.00213265, 0.95799672, 0.94516302]])\n",
      " array([[1.09692578, 1.07148344, 1.12040182, 1.11620492],\n",
      "       [1.14227686, 1.20671747, 1.18748218, 1.12318623],\n",
      "       [1.09692578, 1.04721066, 0.96152727, 1.09526102],\n",
      "       [1.03762052, 0.98132741, 1.04626036, 0.99403214],\n",
      "       [0.98529235, 0.96052217, 1.02154654, 0.99752279],\n",
      "       [0.97831527, 0.98132741, 1.02154654, 0.96610693],\n",
      "       [1.00622362, 1.04374312, 1.05332145, 1.08129841],\n",
      "       [1.04110907, 0.99519757, 1.018016  , 0.96610693],\n",
      "       [0.97831527, 0.96052217, 1.02507709, 0.99752279],\n",
      "       [0.99924653, 0.96745725, 1.00389381, 0.99752279],\n",
      "       [0.97831527, 0.97785987, 1.018016  , 1.02893865],\n",
      "       [1.03064343, 1.06108082, 1.09921854, 1.0463919 ],\n",
      "       [0.9887809 , 0.96052217, 0.9721189 , 0.95563498],\n",
      "       [0.95040691, 0.90504154, 0.91209963, 0.8613874 ]])\n",
      " array([[0.98529235, 0.9535871 , 0.98271054, 0.91374716],\n",
      "       [0.88761311, 0.89463893, 0.91916072, 0.95563498],\n",
      "       [0.95389545, 0.89810647, 0.96505781, 0.94516302],\n",
      "       [0.96087254, 0.95705464, 1.01095491, 1.00450409],\n",
      "       [1.02366634, 0.96745725, 1.03566872, 0.98705084],\n",
      "       [0.99226944, 0.96052217, 1.02860763, 0.96610693],\n",
      "       [0.96436109, 1.00906773, 1.00742436, 1.06384516],\n",
      "       [1.13181123, 1.34541905, 1.20160436, 1.39545701],\n",
      "       [1.3725208 , 1.32461381, 1.35694837, 1.29422813],\n",
      "       [1.35158954, 1.32461381, 1.38166219, 1.32564399],\n",
      "       [1.33414681, 1.26913318, 1.34988728, 1.30470008],\n",
      "       [1.26786447, 1.33501643, 1.32517346, 1.30470008],\n",
      "       [1.32716972, 1.27606826, 1.29692909, 1.25932162],\n",
      "       [1.27135301, 1.23099025, 1.29692909, 1.24186836]])\n",
      " array([[1.10041433, 1.04721066, 1.08509636, 1.07780776],\n",
      "       [1.07250597, 1.13389915, 1.14158509, 1.16158339],\n",
      "       [1.17716231, 1.12002899, 1.14864618, 1.11620492],\n",
      "       [1.14227686, 1.09575621, 1.11687127, 1.06035451],\n",
      "       [1.03413198, 1.02293788, 1.05332145, 1.08478906],\n",
      "       [1.06552888, 1.00213265, 0.95799672, 0.94516302],\n",
      "       [0.89807874, 0.99173003, 0.91209963, 1.0463919 ],\n",
      "       [1.04459761, 1.09575621, 1.09215745, 1.06733581],\n",
      "       [1.09692578, 1.07148344, 1.12040182, 1.11620492],\n",
      "       [1.14227686, 1.20671747, 1.18748218, 1.12318623],\n",
      "       [1.09692578, 1.04721066, 0.96152727, 1.09526102],\n",
      "       [1.03762052, 0.98132741, 1.04626036, 0.99403214],\n",
      "       [0.98529235, 0.96052217, 1.02154654, 0.99752279],\n",
      "       [0.97831527, 0.98132741, 1.02154654, 0.96610693]])\n",
      " array([[1.26786447, 1.33501643, 1.32517346, 1.30470008],\n",
      "       [1.32716972, 1.27606826, 1.29692909, 1.25932162],\n",
      "       [1.27135301, 1.23099025, 1.29692909, 1.24186836],\n",
      "       [1.26088738, 1.20324993, 1.24750146, 1.23837771],\n",
      "       [1.19111649, 1.14083422, 1.22984873, 1.17903664],\n",
      "       [1.20507066, 1.16163946, 1.20160436, 1.14413013],\n",
      "       [1.14925395, 1.13043161, 1.19454327, 1.16507404],\n",
      "       [1.19460503, 1.1477693 , 1.13805454, 1.08827972],\n",
      "       [1.07948306, 1.05761328, 1.10274909, 1.11271427],\n",
      "       [1.09692578, 1.08535359, 1.095688  , 1.07780776],\n",
      "       [1.06552888, 1.03680804, 0.96152727, 0.92421912],\n",
      "       [0.86668184, 0.86689861, 0.90503854, 0.84742479],\n",
      "       [0.88412456, 0.92584678, 0.94387454, 0.99054149],\n",
      "       [0.96436109, 0.92931432, 0.9721189 , 0.93469107]])\n",
      " array([[1.32716972, 1.27606826, 1.29692909, 1.25932162],\n",
      "       [1.27135301, 1.23099025, 1.29692909, 1.24186836],\n",
      "       [1.26088738, 1.20324993, 1.24750146, 1.23837771],\n",
      "       [1.19111649, 1.14083422, 1.22984873, 1.17903664],\n",
      "       [1.20507066, 1.16163946, 1.20160436, 1.14413013],\n",
      "       [1.14925395, 1.13043161, 1.19454327, 1.16507404],\n",
      "       [1.19460503, 1.1477693 , 1.13805454, 1.08827972],\n",
      "       [1.07948306, 1.05761328, 1.10274909, 1.11271427],\n",
      "       [1.09692578, 1.08535359, 1.095688  , 1.07780776],\n",
      "       [1.06552888, 1.03680804, 0.96152727, 0.92421912],\n",
      "       [0.86668184, 0.86689861, 0.90503854, 0.84742479],\n",
      "       [0.88412456, 0.92584678, 0.94387454, 0.99054149],\n",
      "       [0.96436109, 0.92931432, 0.9721189 , 0.93469107],\n",
      "       [0.92947564, 0.92584678, 0.9721189 , 0.90676586]])\n",
      " array([[-1.19853656, -1.17201456, -1.17092224, -1.21205931],\n",
      "       [-1.29970436, -1.34192399, -1.39687716, -1.39706381],\n",
      "       [-1.37994088, -1.36619676, -1.40393825, -1.43197032],\n",
      "       [-1.42180342, -1.4216774 , -1.44277425, -1.48083944],\n",
      "       [-1.48459722, -1.50489834, -1.47807971, -1.49829269],\n",
      "       [-1.4985514 , -1.50836588, -1.47101861, -1.48083944],\n",
      "       [-1.43924614, -1.45982033, -1.41452989, -1.44942358],\n",
      "       [-1.44622323, -1.48756064, -1.45689643, -1.47734879],\n",
      "       [-1.47064304, -1.46675541, -1.42865207, -1.43895163],\n",
      "       [-1.42529196, -1.46675541, -1.4392437 , -1.43546097],\n",
      "       [-1.50203994, -1.42861247, -1.46042698, -1.38310121],\n",
      "       [-1.38342943, -1.41127478, -1.36510225, -1.37611991],\n",
      "       [-1.36947525, -1.36619676, -1.4392437 , -1.45989553],\n",
      "       [-1.44273469, -1.42861247, -1.40393825, -1.41102642]])\n",
      " array([[-0.53571309, -0.49237684, -0.50717968, -0.47902259],\n",
      "       [-0.4345453 , -0.48544176, -0.47893532, -0.51043845],\n",
      "       [-0.55664436, -0.57213024, -0.53542404, -0.55581692],\n",
      "       [-0.52175891, -0.44036374, -0.47187422, -0.38477502],\n",
      "       [-0.26011807, -0.30512971, -0.27063313, -0.27307418],\n",
      "       [-0.31593478, -0.27045431, -0.30240804, -0.23467702],\n",
      "       [-0.13453046, -0.16296059, -0.16118622, -0.21722377],\n",
      "       [-0.25662952, -0.26351923, -0.23885822, -0.24165832],\n",
      "       [-0.24965243, -0.23924646, -0.21061385, -0.20675181],\n",
      "       [-0.17290445, -0.08667473, -0.11528912, -0.09854163],\n",
      "       [ 0.12362184,  0.11444256,  0.07536034,  0.03410311],\n",
      "       [ 0.06082804,  0.01388392,  0.05417706,  0.05155636],\n",
      "       [ 0.06780512,  0.01388392,  0.04358543,  0.01664985],\n",
      "       [ 0.01198841, -0.04506425, -0.07998367, -0.06014447]])\n",
      " array([[-1.29272727, -1.32458629, -1.30861352, -1.31328819],\n",
      "       [-1.3031929 , -1.35232661, -1.3227357 , -1.35168535],\n",
      "       [-1.33807835, -1.34885907, -1.31920516, -1.35168535],\n",
      "       [-1.34156689, -1.35232661, -1.31214406, -1.30630689],\n",
      "       [-1.34156689, -1.32805383, -1.38628552, -1.40055446],\n",
      "       [-1.42529196, -1.42861247, -1.42512152, -1.45291423],\n",
      "       [-1.44971178, -1.41127478, -1.41806043, -1.36564795],\n",
      "       [-1.38342943, -1.34192399, -1.37922443, -1.40753577],\n",
      "       [-1.36947525, -1.37313184, -1.38628552, -1.37262926],\n",
      "       [-1.40784924, -1.44248263, -1.43218261, -1.47036748],\n",
      "       [-1.44971178, -1.49796326, -1.47101861, -1.46338618],\n",
      "       [-1.46017741, -1.49796326, -1.47807971, -1.5192366 ],\n",
      "       [-1.50552849, -1.54997635, -1.50279352, -1.53668985],\n",
      "       [-1.52297121, -1.51183342, -1.49220189, -1.46687683]])\n",
      " array([[-1.50552849, -1.54997635, -1.50279352, -1.53668985],\n",
      "       [-1.52297121, -1.51183342, -1.49220189, -1.46687683],\n",
      "       [-1.46017741, -1.49102818, -1.47101861, -1.48433009],\n",
      "       [-1.48459722, -1.28644336, -1.46042698, -1.25743777],\n",
      "       [-1.23691056, -1.1581444 , -1.24153315, -1.15271824],\n",
      "       [-1.19853656, -1.17201456, -1.17092224, -1.21205931],\n",
      "       [-1.29970436, -1.34192399, -1.39687716, -1.39706381],\n",
      "       [-1.37994088, -1.36619676, -1.40393825, -1.43197032],\n",
      "       [-1.42180342, -1.4216774 , -1.44277425, -1.48083944],\n",
      "       [-1.48459722, -1.50489834, -1.47807971, -1.49829269],\n",
      "       [-1.4985514 , -1.50836588, -1.47101861, -1.48083944],\n",
      "       [-1.43924614, -1.45982033, -1.41452989, -1.44942358],\n",
      "       [-1.44622323, -1.48756064, -1.45689643, -1.47734879],\n",
      "       [-1.47064304, -1.46675541, -1.42865207, -1.43895163]])\n",
      " array([[1.09692578, 1.04721066, 0.96152727, 1.09526102],\n",
      "       [1.03762052, 0.98132741, 1.04626036, 0.99403214],\n",
      "       [0.98529235, 0.96052217, 1.02154654, 0.99752279],\n",
      "       [0.97831527, 0.98132741, 1.02154654, 0.96610693],\n",
      "       [1.00622362, 1.04374312, 1.05332145, 1.08129841],\n",
      "       [1.04110907, 0.99519757, 1.018016  , 0.96610693],\n",
      "       [0.97831527, 0.96052217, 1.02507709, 0.99752279],\n",
      "       [0.99924653, 0.96745725, 1.00389381, 0.99752279],\n",
      "       [0.97831527, 0.97785987, 1.018016  , 1.02893865],\n",
      "       [1.03064343, 1.06108082, 1.09921854, 1.0463919 ],\n",
      "       [0.9887809 , 0.96052217, 0.9721189 , 0.95563498],\n",
      "       [0.95040691, 0.90504154, 0.91209963, 0.8613874 ],\n",
      "       [0.85272766, 0.87383369, 0.8944469 , 0.91374716],\n",
      "       [0.96436109, 0.96398971, 1.00389381, 1.01497604]])\n",
      " array([[0.25269799, 0.36063786, 0.16009343, 0.35873365],\n",
      "       [0.36781996, 0.32943   , 0.31190689, 0.25750477],\n",
      "       [0.22478963, 0.2496766 , 0.17068507, 0.11438808],\n",
      "       [0.17595001, 0.16645565, 0.16009343, 0.13533199],\n",
      "       [0.1829271 , 0.28781953, 0.23070434, 0.30986454],\n",
      "       [0.3085147 , 0.27048183, 0.31190689, 0.29590193],\n",
      "       [0.30502616, 0.33983262, 0.35427343, 0.38316821],\n",
      "       [0.40270541, 0.35717032, 0.40017053, 0.36571495],\n",
      "       [0.38875123, 0.39184571, 0.43194544, 0.43901863],\n",
      "       [0.42014813, 0.37450802, 0.43194544, 0.41458407],\n",
      "       [0.43061376, 0.37104048, 0.4213538 , 0.36571495],\n",
      "       [0.36084287, 0.32943   , 0.35427343, 0.38665886],\n",
      "       [0.36781996, 0.42305357, 0.4213538 , 0.48090644],\n",
      "       [0.51085029, 0.50627451, 0.56257562, 0.56468206]])\n",
      " array([[0.02594259, 0.12484518, 0.08242143, 0.14580394],\n",
      "       [0.16548437, 0.38144309, 0.22717379, 0.35175235],\n",
      "       [0.42014813, 0.3641054 , 0.37192616, 0.32033649],\n",
      "       [0.32595743, 0.31902739, 0.3436818 , 0.34477105],\n",
      "       [0.26316362, 0.35023524, 0.30837634, 0.38316821],\n",
      "       [0.40270541, 0.48200174, 0.43194544, 0.49835969],\n",
      "       [0.49689611, 0.45079388, 0.47078144, 0.42505602],\n",
      "       [0.43759085, 0.41958603, 0.49196471, 0.45647188],\n",
      "       [0.43410231, 0.37450802, 0.37545671, 0.3412804 ],\n",
      "       [0.39223977, 0.39184571, 0.42841489, 0.37618691],\n",
      "       [0.39921686, 0.33983262, 0.39310943, 0.3622243 ],\n",
      "       [0.36433142, 0.31555985, 0.31190689, 0.25052347],\n",
      "       [0.214324  , 0.22887136, 0.25188761, 0.26448607],\n",
      "       [0.27362926, 0.21846874, 0.20599052, 0.22259826]])\n",
      " array([[0.99226944, 0.98132741, 0.98624109, 1.00101344],\n",
      "       [0.97831527, 0.94318448, 0.96858836, 0.96959758],\n",
      "       [0.90854437, 0.92237924, 0.83442763, 0.80553698],\n",
      "       [0.83877348, 0.86689861, 0.8167749 , 0.74968657],\n",
      "       [0.71318588, 0.83569076, 0.72145017, 0.87185935],\n",
      "       [0.90505583, 0.90504154, 0.89091636, 0.92072847],\n",
      "       [0.89459019, 0.87730123, 0.91563018, 0.90676586],\n",
      "       [0.91552146, 0.98479495, 0.95799672, 1.02893865],\n",
      "       [1.05506325, 1.00906773, 1.03919927, 0.96959758],\n",
      "       [1.00273508, 0.98479495, 1.04626036, 1.02195735],\n",
      "       [1.08646015, 1.05761328, 1.12393236, 1.08478906],\n",
      "       [1.0829716 , 1.0333405 , 1.12393236, 1.06384516],\n",
      "       [1.06204034, 1.04374312, 1.08509636, 1.06035451],\n",
      "       [1.01320071, 1.0333405 , 1.02507709, 1.08129841]])\n",
      " array([[-1.33110126, -1.31765121, -1.31214406, -1.28885363],\n",
      "       [-1.28575018, -1.32805383, -1.31920516, -1.355176  ],\n",
      "       [-1.34854398, -1.3696643 , -1.33332734, -1.36564795],\n",
      "       [-1.35552107, -1.3696643 , -1.35451061, -1.38659186],\n",
      "       [-1.4043607 , -1.43554755, -1.40393825, -1.42498902],\n",
      "       [-1.43226905, -1.43554755, -1.39687716, -1.41102642],\n",
      "       [-1.39738361, -1.34539153, -1.37216334, -1.29932558],\n",
      "       [-1.29272727, -1.32458629, -1.30861352, -1.31328819],\n",
      "       [-1.3031929 , -1.35232661, -1.3227357 , -1.35168535],\n",
      "       [-1.33807835, -1.34885907, -1.31920516, -1.35168535],\n",
      "       [-1.34156689, -1.35232661, -1.31214406, -1.30630689],\n",
      "       [-1.34156689, -1.32805383, -1.38628552, -1.40055446],\n",
      "       [-1.42529196, -1.42861247, -1.42512152, -1.45291423],\n",
      "       [-1.44971178, -1.41127478, -1.41806043, -1.36564795]])\n",
      " array([[-1.26830746, -1.25870304, -1.22388043, -1.23300321],\n",
      "       [-1.27528455, -1.26217058, -1.26271643, -1.21554996],\n",
      "       [-1.18458239, -1.09919623, -1.14620842, -1.06894262],\n",
      "       [-1.03457497, -1.05758576, -1.0438226 , -1.06894262],\n",
      "       [-1.04852915, -1.07492346, -1.02970042, -1.05148936],\n",
      "       [-1.07294896, -1.07839099, -1.02970042, -1.05847066],\n",
      "       [-1.05550624, -0.96049465, -1.01204769, -0.91884462],\n",
      "       [-0.84619356, -0.86340355, -0.9273146 , -0.95026048],\n",
      "       [-1.01015516, -1.0263779 , -1.02616988, -1.03752676],\n",
      "       [-1.03108642, -1.08185853, -1.09325024, -1.13526499],\n",
      "       [-0.99271243, -0.92235172, -0.98027278, -1.02356415],\n",
      "       [-0.94038426, -0.96396219, -1.01557824, -1.04101741],\n",
      "       [-1.05201769, -1.03331298, -1.02263933, -1.02356415],\n",
      "       [-0.99620098, -1.01944282, -1.00851715, -0.98865764]])\n",
      " 0.83994114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8745/1560975069.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_input=np.array(temp_input[1:])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8745/1560975069.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#x_input = x_input.reshape((1, n_steps, n_features))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(x_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} day output {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1669\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1671\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1672\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m                **kwargs):\n\u001b[1;32m    230\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    233\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[0;32m-> 1430\u001b[0;31m   return convert_to_tensor_v2(\n\u001b[0m\u001b[1;32m   1431\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   \u001b[0;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m   return convert_to_tensor(\n\u001b[0m\u001b[1;32m   1437\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "#testing the model\n",
    "x_input = testx\n",
    "temp_input=list(x_input)\n",
    "lst_output=[]\n",
    "i=0\n",
    "while(i<10):\n",
    "    \n",
    "    if(len(temp_input)>14):\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        print(\"{} day input {}\".format(i,x_input))\n",
    "        #print(x_input)\n",
    "        #x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        #print(x_input)\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        scaler.inverse_transform(yhat)\n",
    "        print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.append(yhat[0][0])\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "        lst_output.append(yhat[0][0])\n",
    "        i=i+1\n",
    "    else:\n",
    "        #x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(yhat[0])\n",
    "        temp_input.append(yhat[0][0])\n",
    "        lst_output.append(yhat[0][0])\n",
    "        i=i+1\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "print(lst_output)\n",
    "lstm = lst_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c840f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
